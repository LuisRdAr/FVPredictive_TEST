{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def procesar_directorio(initial_path, cadena):\n",
    "    \"\"\"\n",
    "    Busca mediante recursividad en el directorio inicial todos los directorios hasta encontrar un directorio que contenga\n",
    "    la cadena de texto \"cadena\". Una vez encontrado, busca en el directorio todos los ficheros .csv y los concatena en un\n",
    "    único dataframe. Por último, mueve el directorio encontrado a la carpeta \"Procesados\" y devuelve el dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "        initial_path (str): Ruta del directorio inicial.\n",
    "        cadena (str): Cadena de texto a buscar en los nombres de los directorios.\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Dataframe que contiene la concatenación de todos los ficheros .csv encontrados.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    for filename in os.listdir(initial_path):\n",
    "        complete_path = os.path.join(initial_path, filename)\n",
    "        if (os.path.isdir(complete_path)) and (cadena in filename):\n",
    "            for internal_filename in os.listdir(complete_path):\n",
    "                if internal_filename.endswith(\".csv\"):\n",
    "                    csv_path = os.path.join(complete_path, internal_filename)\n",
    "                    try:\n",
    "                        temp_df = pd.read_csv(csv_path, delimiter = \";\")\n",
    "                        if temp_df.shape[1] == 1:\n",
    "                            temp_df = pd.read_csv(csv_path, delimiter = \",\")\n",
    "                    except pd.errors.ParserError:\n",
    "                        temp_df = pd.read_csv(csv_path, delimiter = \",\")\n",
    "                    df = pd.concat([df, temp_df])\n",
    "                    if not os.path.exists(complete_path.replace(\"Nuevos\", \"Procesados\")):\n",
    "                        os.makedirs(complete_path.replace(\"Nuevos\", \"Procesados\"))\n",
    "                    #shutil.move(csv_path, csv_path.replace(\"Nuevos\", \"Procesados\"))   \n",
    "        elif (os.path.isfile(complete_path)) and (cadena in filename):\n",
    "            csv_path = os.path.join(initial_path, filename)\n",
    "            try:\n",
    "                temp_df = pd.read_csv(csv_path, delimiter = \";\")\n",
    "                if temp_df.shape[1] == 1:\n",
    "                    temp_df = pd.read_csv(csv_path, delimiter = \",\")\n",
    "            except pd.errors.ParserError:\n",
    "                temp_df = pd.read_csv(csv_path, delimiter = \",\")\n",
    "            df = pd.concat([df, temp_df])\n",
    "            if not os.path.exists(initial_path.replace(\"Nuevos\", \"Procesados\")):\n",
    "                os.makedirs(initial_path.replace(\"Nuevos\", \"Procesados\"))\n",
    "            #shutil.move(csv_path, csv_path.replace(\"Nuevos\", \"Procesados\"))\n",
    "        elif (os.path.isdir(complete_path)):\n",
    "            partial_df = procesar_directorio(complete_path, cadena)\n",
    "            if not partial_df.empty:\n",
    "                df = pd.concat([df, partial_df])\n",
    "        else:\n",
    "            continue\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # # Carga, desde la ruta de ejecución, de los parámetros para conexión a la base de datos  \n",
    "    # root_path = os.getcwd()\n",
    "    # params = None\n",
    "    # for filename in os.listdir(root_path):\n",
    "    #     if \"params.json\" in filename:\n",
    "    #         with open(os.path.join(root_path, filename)) as f:\n",
    "    #             params = json.load(f)\n",
    "    # if params is None:\n",
    "    #     print(\"No se ha encontrado el archivo de parámetros para la conexión a la base de datos\")\n",
    "    #     sys.exit()\n",
    "    # else:\n",
    "    #     print(f\"Parámetros de la planta {params['schema'].capitalize()} cargados correctamente\")\n",
    "    # data_path = os.path.join(root_path, params[\"data_path\"])\n",
    "    # schema_name = params[\"schema\"]\n",
    "\n",
    "    # Carga y comprobación de la existencia de nuevos ficheros para procesar\n",
    "    data_path = \"/home/upo/Desktop/Test_FVPredictive/FVPredictive_TEST/Alcazar/Datos/Nuevos\"\n",
    "    df = procesar_directorio(data_path, 'IAZ')\n",
    "    if df.empty:\n",
    "        print(f\"No se han encontrado nuevos ficheros para procesar en {data_path}\")\n",
    "        sys.exit()\n",
    "    print(f\"Se han encontrado {df.shape[0]} nuevos registros para procesar\")\n",
    "\n",
    "    columnas = df.columns\n",
    "\n",
    "    # Crear un diccionario para almacenar grupos de columnas y los nombres de las columnas para el dataframe reestructurado\n",
    "    grupos_columnas = {}\n",
    "    col_names = []\n",
    "\n",
    "    # Iterar sobre las columnas y agruparlas por el patrón común\n",
    "    for columna in columnas:\n",
    "        if columna != \"Datetime\":\n",
    "            if columna.split(\" - \")[1] not in col_names:\n",
    "                col_names.append(columna.split(\" - \")[1])\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "            grupo = columna.split(\" - \")[0]\n",
    "            if grupo not in grupos_columnas:\n",
    "                grupos_columnas[grupo] = [columna]\n",
    "            else:\n",
    "                grupos_columnas[grupo].append(columna)\n",
    "\n",
    "    # Crear un dataframe para cada grupo de columnas y concatenarlos en un único dataframe\n",
    "    meteo_df = pd.DataFrame()\n",
    "    for grupo, columnas_grupo in grupos_columnas.items():\n",
    "        single_meteo_df = pd.concat([df[\"Datetime\"], df[columnas_grupo].copy().rename(columns={col: col.split(\" - \")[1] for col in columnas_grupo})], axis = 1)\n",
    "        single_meteo_df[\"disp_name\"] = grupo\n",
    "        meteo_df = pd.concat([meteo_df, single_meteo_df], axis=0)\n",
    "\n",
    "    # Asignación de identificadores a los dispositivos y parque\n",
    "    meteo_df[\"parque_id\"] = meteo_df[\"disp_name\"].str[3]\n",
    "    meteo_df[\"dispositivo_id\"] = meteo_df[\"dispositivo_id\"] = meteo_df.groupby(\"parque_id\")[\"disp_name\"].transform(lambda x: x.astype(\"category\").cat.codes + 41)\n",
    "    meteo_df.drop(\"disp_name\", axis=1, inplace=True)\n",
    "\n",
    "    # Procesado de columnas: conversión de tipos\n",
    "    columns = []\n",
    "    for column in meteo_df.columns:\n",
    "        try:\n",
    "            meteo_df[column] = meteo_df[column].str.replace(\",\", \".\").astype(float)\n",
    "            if meteo_df[column].iloc[0].is_integer():\n",
    "                meteo_df[column] = meteo_df[column].astype(int)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        columns.append(column.strip().replace(\" \", \"_\").split(\"(\")[0].lower())\n",
    "    meteo_df.columns = columns\n",
    "\n",
    "    meteo_df = meteo_df.rename(columns={'datetime': \"datetime_utc\", \n",
    "                                'wind_speed_1': \"vel_viento\", \n",
    "                                'wind_direction_1': \"dir_viento\",\n",
    "                                'average_external_ambient_temperature': \"temp_amb\", \n",
    "                                'average_humidity': \"hum_rel\",\n",
    "                                'average_atmospheric_pressure': \"presion_atm\", \n",
    "                                'radiation_pyranometer_1': \"rad_hor\",\n",
    "                                'radiation_pyranometer_2': \"rad_poa\"})\n",
    "\n",
    "    # Parseo de fechas y ordenación del dataframe\n",
    "    meteo_df[\"datetime_utc\"] = pd.to_datetime(meteo_df[\"datetime_utc\"])\n",
    "    num_duplicates = meteo_df[meteo_df.duplicated(subset = [\"datetime_utc\", \"dispositivo_id\"])].shape[0]\n",
    "    meteo_df = meteo_df.drop_duplicates(subset = [\"datetime_utc\", \"dispositivo_id\"], keep = \"last\").reset_index(drop = True)\n",
    "\n",
    "    # Conexión a la base de datos y carga de datos\n",
    "    try:\n",
    "        password = params['password'].replace('@', '%40')\n",
    "        engine = create_engine(f'postgresql://{params[\"user\"]}:{password}@{params[\"host\"]}:{params[\"port\"]}/{params[\"dbname\"]}')\n",
    "        print(f\"Conexión a la base de datos {params['dbname']} (esquema {schema_name}) establecida\")\n",
    "    except Exception as error:\n",
    "        print(\"Error en la apertura de base de datos: \\n\\t{}\".format(error))\n",
    "        sys.exit()\n",
    "\n",
    "    # Comprobación de la existencia de registros duplicados en la base de datos\n",
    "    check_query = f\"SELECT datetime_utc, dispositivo_id FROM {schema_name}.meteo_raw\"\n",
    "    check_df = pd.read_sql_query(check_query, engine)\n",
    "    check_df[\"datetime_utc\"] = pd.to_datetime(check_df[\"datetime_utc\"], utc=True)\n",
    "    merged_df = meteo_df.merge(check_df, how='left', indicator=True, left_on = [\"datetime_utc\", \"dispositivo_id\"], right_on = [\"datetime_utc\", \"dispositivo_id\"])\n",
    "    meteo_df = merged_df[merged_df[\"_merge\"] == \"left_only\"]\n",
    "    meteo_df = meteo_df.drop(columns = \"_merge\")\n",
    "\n",
    "    print(f\"Se han encontrado {num_duplicates} registros duplicados\")\n",
    "    print(f\"Se han encontrado {merged_df.shape[0] - meteo_df.shape[0]} registros ya existentes en la base de datos\")\n",
    "\n",
    "    # Ordenación del dataframe por fecha y dispositivo\n",
    "    meteo_df = meteo_df.sort_values(by = [\"datetime_utc\", \"dispositivo_id\"])\\\n",
    "            .reset_index(drop = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Test_FVPredictive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
