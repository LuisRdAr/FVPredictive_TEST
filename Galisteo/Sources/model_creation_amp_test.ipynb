{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sqlalchemy import create_engine\n",
    "import warnings\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials, space_eval\n",
    "from functools import partial\n",
    "from sklearn.metrics import mean_squared_error as mse, \\\n",
    "                            mean_absolute_error as mae, \\\n",
    "                            r2_score as r2\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "def discriminador(row):\n",
    "    mean = np.mean(row)\n",
    "    std = np.std(row)\n",
    "    threshold = 3 * std\n",
    "    outlier = np.abs(row - mean) > threshold\n",
    "    return outlier\n",
    "\n",
    "class MultiOutputOpt(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_class=XGBRegressor, device=\"cpu\", trials=None):\n",
    "        self.model_class = model_class\n",
    "        self.device = device\n",
    "        self.models = {}  \n",
    "        self.trials = trials if trials is not None else {}\n",
    "        try:\n",
    "            print(len(self.trials[0].trials))\n",
    "        except:\n",
    "            pass\n",
    "        self.params = {}\n",
    "        \n",
    "    def objective(self, space_params, train_set, cv_folds):\n",
    "        n_estimators = int(space_params.pop('n_estimators'))\n",
    "        params = {'device': self.device,\n",
    "            'objective': 'reg:squarederror', \n",
    "            'tree_method': \"hist\", \n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1}\n",
    "        params = {**params, **space_params}\n",
    "        if 'max_depth' in params:\n",
    "            params['max_depth'] = int(params['max_depth'])\n",
    "        if 'min_child_weight' in params:\n",
    "            params['min_child_weight'] = max(1, int(params['min_child_weight']))\n",
    "        print\n",
    "        cv_result = xgb.cv(params, train_set, nfold = cv_folds, num_boost_round = n_estimators, early_stopping_rounds = 100, metrics = 'rmse', as_pandas = True)\n",
    "        score = cv_result['test-rmse-mean'].min()\n",
    "        return {'loss': score, 'status': STATUS_OK}\n",
    "        \n",
    "    def optimize(self, X, y, space, cv_folds, gamma_algo = 1, STALL_LIMIT = 5, MAX_EVALS_PER_RUN = 250):\n",
    "        for col in tqdm(range(y.shape[1]), total = y.shape[1]):\n",
    "            if self.params.get(col) is not None:\n",
    "                space.update(self.params[col])\n",
    "            train_set = xgb.DMatrix(X, label=y[:, col])\n",
    "            trials = Trials()\n",
    "            TOTAL_EVALS = len(trials.trials)\n",
    "            STALL_LIMIT = STALL_LIMIT\n",
    "            MAX_EVALS_PER_RUN = MAX_EVALS_PER_RUN\n",
    "            best_loss = np.inf\n",
    "            stall_counter = 0\n",
    "            num_evals = TOTAL_EVALS\n",
    "            run_counter = 0\n",
    "            upper_limit = (MAX_EVALS_PER_RUN * (STALL_LIMIT - 1)) * 10\n",
    "            while stall_counter < STALL_LIMIT and num_evals < TOTAL_EVALS + upper_limit:\n",
    "                best = fmin(fn=lambda space: self.objective(space, train_set = train_set, cv_folds = cv_folds), \n",
    "                        space = space, \n",
    "                        algo = partial(tpe.suggest, gamma = gamma_algo),\n",
    "                        max_evals = num_evals + MAX_EVALS_PER_RUN, \n",
    "                        trials = trials,\n",
    "                        verbose = False)  # Cambiado a True para que devuelva los parámetros óptimos\n",
    "                best_params = space_eval(space, best)  # Obtener los parámetros óptimos en su forma original\n",
    "                new_loss = trials.best_trial['result']['loss']\n",
    "                if new_loss < best_loss:\n",
    "                    threshold = 0.001\n",
    "                    if abs(new_loss - best_loss) <= threshold:\n",
    "                        stall_counter += 1\n",
    "                    else:\n",
    "                        stall_counter = 0\n",
    "                    best_loss = new_loss\n",
    "                else:\n",
    "                    stall_counter += 1\n",
    "                num_evals += MAX_EVALS_PER_RUN\n",
    "                run_counter += 1\n",
    "                gamma_algo -= 0.05\n",
    "            print(f\"\\tEntrenamiento para entrada {col+1} finalizado\")\n",
    "            print(f\"\\tNúmero de evaluaciones realizadas: {num_evals}\")\n",
    "            print(f\"\\tBest params: {best_params}\")\n",
    "            print(f\"\\tBest loss: {best_loss}\")\n",
    "\n",
    "            final_params = {\"device\": self.device,\n",
    "                        \"objective\": 'reg:squarederror', \n",
    "                        \"tree_method\": \"hist\",\n",
    "                        \"n_jobs\": -1}\n",
    "            final_params = {**final_params, **best_params}\n",
    "            final_params.pop('n_estimators')\n",
    "            if 'max_depth' in final_params:\n",
    "                final_params['max_depth'] = int(final_params['max_depth'])\n",
    "            if 'min_child_weight' in final_params:\n",
    "                final_params['min_child_weight'] = max(1, int(final_params['min_child_weight']))\n",
    "            model = xgb.train(final_params, train_set, num_boost_round = int(best_params['n_estimators']))\n",
    "            self.models[col] = model\n",
    "            self.trials[col] = trials\n",
    "            self.params[col] = best_params  # Guardar los parámetros óptimos en su forma original\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        dmatrix = xgb.DMatrix(X)\n",
    "        for col, model in self.models.items():\n",
    "            pred = model.predict(dmatrix)\n",
    "            predictions.append(pred)\n",
    "        return np.column_stack(predictions)\n",
    "        \n",
    "    \n",
    "print(\"Configuración para el entrenamiento de modelos de predicción de entrada de corriente continua en inversores fotovoltaicos:\", end=\"\\n\\n\")\n",
    "# valid_models = {\"1\": \"XGBRegressor\", \"2\": \"RandomForestRegressor\"}\n",
    "# model_name = \"\"\n",
    "# while model_name not in valid_models.keys():\n",
    "#     model_name = input(\"Ingrese el valor numérico para el tipo de modelo que desea utilizar (XGBRegressor[1], RandomForestRegressor[2]): \")\n",
    "# model_name = valid_models[model_name]\n",
    "model_name = \"XGBRegressor\"\n",
    "\n",
    "valid_responses = [\"y\", \"n\"]\n",
    "\n",
    "promediado = \"\"\n",
    "intervalo = 0\n",
    "intervalos = [5, 10, 15, 20, 30, 60]\n",
    "while promediado not in valid_responses:\n",
    "    promediado = input(\"¿Desea promediar los datos de entrada? (Y/N): \").lower()\n",
    "promediado = promediado == \"y\"\n",
    "if promediado:\n",
    "    while intervalo not in intervalos:\n",
    "        intervalo = int(input(\"\\tIngrese el intervalo de tiempo en minutos [5, 10, 15, 20, 30, 60] para el promediado de los datos de entrada: \"))\n",
    "\n",
    "normalizacion = \"\"\n",
    "while normalizacion not in valid_responses:\n",
    "    normalizacion = input(\"¿Desea normalizar el target? (Y/N): \").lower()\n",
    "normalizacion = normalizacion == \"y\"\n",
    "\n",
    "optimizacion = \"\"\n",
    "while optimizacion not in valid_responses:\n",
    "    optimizacion = input(\"¿Desea optimizar el modelo? (Y/N): \").lower()\n",
    "optimizacion = optimizacion == \"y\"\n",
    "\n",
    "stage = True\n",
    "valid_devices = [\"cpu\", \"cuda\"]\n",
    "device = \"\"\n",
    "if optimizacion:\n",
    "    while stage not in valid_responses:\n",
    "        stage = input(\"\\t¿Desea optimizar por fases? (Y/N): \").lower()\n",
    "    stage = stage == \"y\"\n",
    "    # Comprobación de la disponibilidad de GPU para el entrenamiento\n",
    "    if torch.cuda.is_available():\n",
    "        while device not in valid_devices:\n",
    "            device_bool = input(\"¿Desea utilizar GPU para el entrenamiento? (Y/N): \").lower()\n",
    "            if device_bool == \"y\":\n",
    "                device = \"cuda\"\n",
    "            else:\n",
    "                device = \"cpu\"\n",
    "else:\n",
    "    stage = False\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"\\nLas opciones seleccionadas son: \\nModelo: {model_name} \\\n",
    "      \\nPromediado: {intervalo} min \\\n",
    "      \\nNormalización: {normalizacion} \\\n",
    "      \\nOptimización: {optimizacion} \\\n",
    "      \\nEntrenamiento: {device}\", end=\"\\n\\n\")\n",
    "\n",
    "root_path = \"/home/upo/Desktop/Test_FVPredictive/FVPredictive_TEST/Galisteo\"\n",
    "params = None\n",
    "for filename in os.listdir(root_path):\n",
    "    if \"params.json\" in filename:\n",
    "        with open(os.path.join(root_path, filename)) as f:\n",
    "            params = json.load(f)\n",
    "if params is None:\n",
    "    print(\"No se ha encontrado el archivo de parámetros para la conexión a la base de datos\")\n",
    "    sys.exit()\n",
    "data_path = os.path.join(root_path, params[\"data_path\"])\n",
    "schema_name = params[\"schema\"]\n",
    "\n",
    "password = params['password'].replace('@', '%40')\n",
    "engine = create_engine(f'postgresql://{params[\"user\"]}:{password}@{params[\"host\"]}:{params[\"port\"]}/{params[\"dbname\"]}')\n",
    "print(f\"Conexión a la base de datos {params['dbname']} (esquema {schema_name}) establecida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mod_string = 30\n",
    "sup_mod = 2\n",
    "# Carga de los datos de entrenamiento\n",
    "if promediado:\n",
    "    main_query = f\"\"\"\n",
    "            WITH inv AS (\n",
    "                SELECT \n",
    "                    date_trunc('hour', datetime_utc) + \n",
    "                    INTERVAL '{intervalo} min' * floor(date_part('minute', datetime_utc) / {intervalo}) as datetime_utc_rounded,\n",
    "                    dispositivo_id,\n",
    "                    MIN(inv.id) AS id, \n",
    "                    AVG(potencia_act) as potencia_act, \n",
    "                    AVG(amp_dc) as amp_dc,\n",
    "                    det.entrada_id\n",
    "                FROM {schema_name}.inversores AS inv\n",
    "                JOIN {schema_name}.inversores_detalle AS det\n",
    "                    ON inv.id = det.id\n",
    "                WHERE (dispositivo_id != 26)\n",
    "                    AND (alarma = 0)\n",
    "                    AND (estado = 6)\n",
    "                GROUP BY datetime_utc_rounded, dispositivo_id, det.entrada_id \n",
    "            ),\n",
    "            met AS (\n",
    "                SELECT \n",
    "                    date_trunc('hour', datetime_utc) + \n",
    "                    INTERVAL '{intervalo} min' * floor(date_part('minute', datetime_utc) / {intervalo}) as datetime_utc_rounded, \n",
    "                    dispositivo_id,\n",
    "                    AVG(rad_poa) AS rad_poa, \n",
    "                    AVG(rad_hor) AS rad_hor, \n",
    "                    AVG(rad_celda1) AS rad_celda1,\n",
    "                    AVG(rad_celda2) AS rad_celda2, \n",
    "                    AVG(temp_amb) AS temp_amb, \n",
    "                    AVG(temp_panel1) AS temp_panel1,\n",
    "                    AVG(temp_panel2) AS temp_panel2, \n",
    "                    AVG(cloud_impact) AS cloud_impact,\n",
    "                    BOOL_OR(daylight) AS daylight\n",
    "                FROM {schema_name}.meteo\n",
    "                    WHERE daylight = true\n",
    "                GROUP BY dispositivo_id, datetime_utc_rounded\n",
    "            )\n",
    "            SELECT \n",
    "                inv.id,\n",
    "                inv.dispositivo_id,\n",
    "                inv.entrada_id,\n",
    "                inv.datetime_utc_rounded as datetime_utc, \n",
    "                potencia_act,  \n",
    "                num_strings, \n",
    "                rad_poa,\n",
    "                rad_hor, \n",
    "                rad_celda1, \n",
    "                rad_celda2, \n",
    "                temp_amb, \n",
    "                temp_panel1, \n",
    "                temp_panel2,\n",
    "                cloud_impact,\n",
    "                motivo,\n",
    "                consigna_pot_act_planta,\n",
    "                amp_dc\n",
    "            FROM inv\n",
    "            JOIN {schema_name}.distrib_inversores dist\n",
    "                ON dist.dispositivo_id = inv.dispositivo_id\n",
    "                    AND dist.entrada_id = inv.entrada_id\n",
    "            JOIN {schema_name}.dispositivos AS disp\n",
    "                ON disp.dispositivo_id = inv.dispositivo_id\n",
    "            JOIN met\n",
    "                ON met.datetime_utc_rounded = inv.datetime_utc_rounded\n",
    "                    AND met.dispositivo_id = disp.meteo_cercana_id\n",
    "            JOIN {schema_name}.ree AS ree\n",
    "                ON ree.datetime_utc = inv.datetime_utc_rounded\n",
    "            ORDER BY 5, 2, 3, 4;\"\"\"\n",
    "else:\n",
    "    main_query = f\"\"\"\n",
    "        WITH f AS (\n",
    "            SELECT *\n",
    "                FROM {schema_name}.inversores\n",
    "                WHERE (EXTRACT(MINUTE FROM datetime_utc) %% 5 = 0)\n",
    "                    AND (EXTRACT(SECOND FROM datetime_utc) = 0)\n",
    "                    AND (EXTRACT(MONTH FROM datetime_utc) != 10)\n",
    "                    AND (alarma = 0)\n",
    "                    AND (estado = 6)\n",
    "                ORDER BY datetime_utc)\n",
    "        SELECT f.id, f.dispositivo_id, det.entrada_id, f.datetime_utc, potencia_act, num_strings, \n",
    "                rad_poa, rad_hor, rad_celda1, rad_celda2, temp_amb, temp_panel1, temp_panel2, cloud_impact, \n",
    "                motivo, consigna_pot_act_planta, amp_dc\n",
    "            FROM f\n",
    "            JOIN {schema_name}.inversores_detalle AS det\n",
    "                ON f.id = det.id\n",
    "            JOIN {schema_name}.distrib_inversores dist\n",
    "                ON  dist.dispositivo_id = f.dispositivo_id\n",
    "                    AND dist.entrada_id = det.entrada_id\n",
    "            JOIN {schema_name}.dispositivos AS disp\n",
    "                ON disp.dispositivo_id = f.dispositivo_id\n",
    "            JOIN {schema_name}.meteo AS met\n",
    "                ON met.dispositivo_id = disp.meteo_cercana_id\n",
    "                    AND met.datetime_utc = f.datetime_utc\n",
    "            JOIN {schema_name}.ree AS ree\n",
    "                ON ree.datetime_utc = f.datetime_utc\n",
    "            WHERE daylight = true\n",
    "            ORDER BY 4, 2, 3;\"\"\"\n",
    "    \n",
    "chunksize = 100000\n",
    "chunks = pd.read_sql_query(main_query, engine, chunksize=chunksize)\n",
    "main_df = pd.DataFrame()\n",
    "for chunk in chunks:\n",
    "    main_df = pd.concat([main_df, chunk], ignore_index = True)\n",
    "del chunks, chunk\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalización de la entrada de corriente continua, formateo de fechas y escalado de potencia\n",
    "print(f\"Carga inicial de {main_df.shape[0]} registros\")\n",
    "if normalizacion:\n",
    "    print(\"Entrada de corriente continua normalizada según el número de strings\")\n",
    "    main_df[\"amp_dc\"] = main_df[\"amp_dc\"]/main_df[\"num_strings\"]\n",
    "else:\n",
    "    print(\"Entrada de corriente continua sin normalizar\")\n",
    "main_df[\"datetime_utc\"] = pd.to_datetime(main_df[\"datetime_utc\"], utc = True)\n",
    "main_df[\"potencia_act\"] = main_df[\"potencia_act\"] * 1000\n",
    "main_df = main_df.sort_values(by = [\"datetime_utc\", \"dispositivo_id\", \"entrada_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivotado de las entradas de corriente continua\n",
    "target_df = main_df.pivot(index=[\"dispositivo_id\", \"datetime_utc\"], columns='entrada_id', values='amp_dc')\n",
    "target_df.columns = [\"amp_dc_\" + str(col) for col in target_df.columns]\n",
    "print(f\"Número de registros del dataframe tras pivotar: {target_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descarte de registros con corriente anómala\n",
    "target_df[\"outlier\"] = target_df.apply(discriminador, axis=1).any(axis=1)\n",
    "n_corriente_outlier = target_df[target_df[\"outlier\"]].shape[0]\n",
    "target_df = target_df[~target_df[\"outlier\"]].drop(columns=\"outlier\")\n",
    "print(f\"Registros descartados por corrientes anómalas ingresando en el inversor: {n_corriente_outlier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descarte de registros con corriente negativa\n",
    "q1 = main_df[main_df['amp_dc'] < 0]['amp_dc'].quantile(0.25)\n",
    "q3 = main_df[main_df['amp_dc'] < 0]['amp_dc'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "n_corriente_neg = target_df[target_df < lower_bound].dropna(how='all').shape[0]\n",
    "target_df = target_df[target_df >= lower_bound].dropna(how='any')\n",
    "target_df[(target_df >= lower_bound) & (target_df <= 0)] = 0\n",
    "main_df = main_df.drop(columns=[\"entrada_id\", \"amp_dc\"]).drop_duplicates(subset=[\"id\", \"datetime_utc\"]).set_index([\"dispositivo_id\", \"datetime_utc\"])\n",
    "main_df = main_df.merge(target_df, left_index=True, right_index=True, how=\"inner\").sort_index()\n",
    "del target_df\n",
    "gc.collect()\n",
    "print(f\"Registros descartados por corriente negativa: {n_corriente_neg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Búsqueda de outliers basándose en la potencia activa y la potencia solar\n",
    "# num_strings_inv = f\"\"\"SELECT dispositivo_id, SUM(num_strings) as num_strings \n",
    "#                         FROM {schema_name}.distrib_inversores\n",
    "#                         GROUP BY dispositivo_id;\"\"\"\n",
    "# num_strings_inv = pd.read_sql_query(num_strings_inv, engine).sort_values(by=\"dispositivo_id\")\n",
    "# potencia_df = pd.merge(main_df.reset_index()[[\"dispositivo_id\", \"datetime_utc\", \"potencia_act\", \"rad_poa\"]], num_strings_inv, on=\"dispositivo_id\").set_index([\"dispositivo_id\", \"datetime_utc\"])\n",
    "# potencia_df[\"potencia_solar\"] = potencia_df[\"rad_poa\"] * potencia_df[\"num_strings\"] * num_mod_string * sup_mod\n",
    "# potencia_df[\"outlier_solar\"] = np.where(potencia_df[\"potencia_act\"] > 0.20 * potencia_df[\"potencia_solar\"], True, False)\n",
    "# main_df = main_df.merge(potencia_df[[\"outlier_solar\"]], left_index=True, right_index=True, how=\"inner\")\n",
    "# print(f\"Registros descartados por outlier de potencia: {main_df[main_df['outlier_solar'] == True].shape[0]}\")\n",
    "# main_df = main_df[main_df[\"outlier_solar\"] == False].drop(columns = [\"outlier_solar\"])\n",
    "# del potencia_df, num_strings_inv\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Descarte de registros con potencia activa negativa\n",
    "# n_potencia_neg = main_df[main_df[\"potencia_act\"] < 0].shape[0]\n",
    "# main_df = main_df[main_df[\"potencia_act\"] >= 0]\n",
    "# print(f\"Registros descartados por potencia activa negativa: {n_potencia_neg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulación de variables meteorológicas\n",
    "main_df[\"rad_diff\"] = (main_df[\"rad_celda1\"] - main_df[\"rad_celda2\"])\n",
    "main_df[\"temp_panel\"] = (main_df[\"temp_panel1\"] + main_df[\"temp_panel2\"]) / 2\n",
    "main_df = main_df.drop(columns = [\"rad_celda1\", \"rad_celda2\", \"temp_panel1\", \"temp_panel2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulación de variables de consigna\n",
    "main_df[\"motivo\"] = main_df[\"motivo\"].apply(lambda x: 0 if x == 0 else (2 if x == 7 else 1))\n",
    "main_query = f\"\"\"\n",
    "    SELECT MAX(consigna_pot_act_ree)\n",
    "        FROM {schema_name}.ree AS ree;\"\"\"\n",
    "max_pot_act = pd.read_sql_query(main_query, engine).values[0][0]\n",
    "main_df[\"consigna_pot_act_planta\"] = main_df[\"consigna_pot_act_planta\"] / max_pot_act \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignación de variables temporales\n",
    "main_df[\"dia_año\"] = main_df.index.get_level_values(\"datetime_utc\").dayofyear\n",
    "main_df[\"dia_año_sen\"] = np.sin(main_df[\"dia_año\"] * (2*np.pi/365))\n",
    "main_df[\"hora_seg\"] = main_df.index.get_level_values(\"datetime_utc\").hour * 3600 + \\\n",
    "                        main_df.index.get_level_values(\"datetime_utc\").minute * 60 + \\\n",
    "                        main_df.index.get_level_values(\"datetime_utc\").second\n",
    "main_df[\"hora_seg_sen\"] = np.sin(main_df[\"hora_seg\"] * (2*np.pi/86400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado de variables porcentuales\n",
    "main_df[['cloud_impact']] = main_df[['cloud_impact']].apply(lambda x: x/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_power_model(initial_path):\n",
    "    models = {}\n",
    "    for inv_dir in os.listdir(initial_path):\n",
    "        if (\"inversor\" in inv_dir.lower()) & (os.path.isdir(os.path.join(initial_path, inv_dir))):\n",
    "            explotacion_path = os.path.join(initial_path, inv_dir, \"Explotacion\")\n",
    "            for model_dir in os.listdir(explotacion_path):\n",
    "                model_path = os.path.join(explotacion_path, model_dir)\n",
    "                if os.path.isdir(model_path):\n",
    "                    model_dict = {}\n",
    "                    with open(os.path.join(model_path, \"model.model\"), \"rb\") as f:\n",
    "                        model_dict[\"model\"] = pickle.load(f)\n",
    "                    with open(os.path.join(model_path, \"informe_modelo.json\"), \"r\") as f:\n",
    "                        informe_modelo = json.load(f)\n",
    "                        model_dict[\"intervalo\"] = informe_modelo[\"intervalo_min\"]\n",
    "                        model_dict[\"normalizacion\"] = informe_modelo[\"normalizacion\"]\n",
    "                        model_dict[\"RMSE\"] = informe_modelo[\"metricas\"][\"RMSE\"]\n",
    "                    models[inv_dir] = model_dict\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_models = get_power_model(os.path.join(root_path, \"Modelos\", \"potencia_inversor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(power_models.keys())\n",
    "for inv_id in [21]:\n",
    "    print(f\"Dispositivo {inv_id}\")\n",
    "    with open(os.path.join(\"/home/upo/Desktop/Test_FVPredictive/FVPredictive_TEST/Galisteo/Modelos/potencia_inversor/Inversor_1/Repositorio/Booster-2024-02-13 11:46:16.152881\", \"model.model\"), \"rb\") as f:\n",
    "        power_model = pickle.load(f)\n",
    "    disp_df = main_df[main_df.index.get_level_values(\"dispositivo_id\") == inv_id].copy()\n",
    "    print(disp_df.shape)\n",
    "    disp_df = disp_df.dropna()\n",
    "    power_df = disp_df[[\"rad_poa\", \"rad_hor\", \"rad_diff\", \"temp_amb\", \"temp_panel\", \"cloud_impact\", \"motivo\", \"consigna_pot_act_planta\", \"dia_año_sen\", \"hora_seg_sen\"]]\n",
    "    power_prep_df = power_model.named_steps[\"preprocessor\"].transform(power_df)\n",
    "    dtrain = xgb.DMatrix(power_prep_df)\n",
    "power_model.named_steps[\"regressor\"].predict(xgb.DMatrix(power_prep_df))\n",
    "#power_model.named_steps[\"regressor\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for inv_id in np.sort(main_df.index.get_level_values(\"dispositivo_id\").unique()):\n",
    "power_model = get_power_model(root_path)\n",
    "for inv_id in [21]:\n",
    "    print(f\"Dispositivo {inv_id}\")\n",
    "    power_model = power_models[f\"Inversor_{inv_id - 20}\"]\n",
    "    disp_df = main_df[main_df.index.get_level_values(\"dispositivo_id\") == inv_id].copy()\n",
    "    disp_df = disp_df.dropna()\n",
    "\n",
    "    power_df = disp_df[[\"\"]]\n",
    "\n",
    "    # Descarte de valores anómalos en la potencia activa de salida mediante modelo de regresión\n",
    "    power_model = get_power_model(root_path)\n",
    "\n",
    "    # Separación de datos de entrenamiento y validación\n",
    "    train_df, validation_df = train_test_split(disp_df, test_size = 0.2, random_state = 42)\n",
    "\n",
    "    print(f\"\\tRegistros de entrenamiento: {train_df.shape[0]}\")\n",
    "    print(f\"\\tRegistros de validación: {validation_df.shape[0]}\")\n",
    "\n",
    "    # Separación de input y target\n",
    "    target_columns = train_df.filter(like=\"amp_dc\").columns.tolist()\n",
    "    y = train_df[target_columns].copy()\n",
    "    y_val = validation_df[target_columns].copy()\n",
    "    X = train_df.drop(columns = target_columns+[\"id\",\n",
    "                                    \"potencia_act\",\n",
    "                                    \"num_strings\",\n",
    "                                    \"dia_año\",\n",
    "                                    \"hora_seg\"\n",
    "                                    ])\n",
    "    X_val = validation_df.drop(columns = target_columns+[\"id\",\n",
    "                                    \"potencia_act\",\n",
    "                                    \"num_strings\",\n",
    "                                    \"dia_año\",\n",
    "                                    \"hora_seg\"\n",
    "                                    ])\n",
    "    \n",
    "    # Estandarización/normalización de variables numéricas y codificación de variables categóricas\n",
    "    perc_attr = ['cloud_impact', 'consigna_pot_act_planta']\n",
    "    std_attr = ['rad_poa', 'rad_hor', 'temp_amb', 'rad_diff', 'temp_panel']\n",
    "    cat_attr = ['motivo']\n",
    "\n",
    "    transformador_categorico = Pipeline([('onehot', OneHotEncoder(handle_unknown = 'ignore'))])                         # Introducir manualmente catergorías?\n",
    "    transformador_numerico_std = Pipeline([('std_scaler', StandardScaler())]) \n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=[('cat', transformador_categorico, cat_attr),\n",
    "                                                ('std', transformador_numerico_std, std_attr)],\n",
    "                                remainder='passthrough')\n",
    "    \n",
    "    X_prep = preprocessor.fit_transform(X)\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_prep, label=y)\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'tree_method': \"hist\",\n",
    "        'multi_strategy': \"multi_output_tree\",\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    multioutput_model = xgb.train(params, dtrain) \n",
    "\n",
    "    pipeline_model = Pipeline([('preprocessor', preprocessor),\n",
    "                            ('regressor', multioutput_model)])\n",
    "\n",
    "    consulta_sql = f\"\"\"SELECT num_strings\n",
    "                    FROM {schema_name}.distrib_inversores\n",
    "                    WHERE dispositivo_id = {inv_id};\"\"\"\n",
    "    num_strings = pd.read_sql_query(consulta_sql, engine).values.reshape(1, -1)\n",
    "\n",
    "    if optimizacion:\n",
    "        X_val_prep = pipeline_model.named_steps['preprocessor'].transform(X_val)\n",
    "    else:\n",
    "        X_val_prep = xgb.DMatrix(pipeline_model.named_steps['preprocessor'].transform(X_val))\n",
    "    y_pred_val = pd.DataFrame(pipeline_model.named_steps['regressor'].predict(X_val_prep)).rename(columns={i: \"y_pred_\"+str(i+1) for i in pd.DataFrame(pipeline_model.named_steps['regressor'].predict(X_val_prep)).columns})\n",
    "    if normalizacion:\n",
    "        y_pred_val = y_pred_val * num_strings\n",
    "        y_val_reesc = y_val * num_strings\n",
    "    else:\n",
    "        y_pred_val = y_pred_val\n",
    "        y_val_reesc = y_val\n",
    "    \n",
    "    target_pred_df = pd.concat([X_val.reset_index(), y_pred_val], axis=1)[[\"datetime_utc\", \"dispositivo_id\"]+y_pred_val.columns.to_list()] \\\n",
    "                            .melt(id_vars=[\"datetime_utc\", \"dispositivo_id\"], var_name=\"entrada_id\", value_name=\"y_pred\")\n",
    "    target_pred_df[\"entrada_id\"] = target_pred_df[\"entrada_id\"].str.split(\"_\").str[2].astype(int)\n",
    "\n",
    "    y_val_reesc = y_val_reesc.reset_index().melt(id_vars=[\"datetime_utc\", \"dispositivo_id\"], var_name=\"entrada_id\", value_name=\"amp_dc\")\n",
    "    y_val_reesc[\"entrada_id\"] = y_val_reesc[\"entrada_id\"].str.split(\"_\").str[2].astype(int)\n",
    "\n",
    "    prediction_df = target_pred_df.merge(y_val_reesc, on=[\"datetime_utc\", \"dispositivo_id\", \"entrada_id\"])\n",
    "\n",
    "    # Cálculo de las métricas de error\n",
    "    rmse_score = round(mse(prediction_df[\"amp_dc\"], prediction_df[\"y_pred\"], squared=False), 2)\n",
    "    mae_score = round(mae(prediction_df[\"amp_dc\"], prediction_df[\"y_pred\"]), 2)\n",
    "    r2_score = round(r2(prediction_df[\"amp_dc\"], prediction_df[\"y_pred\"]), 3)\n",
    "    metricas = {\"RMSE\": rmse_score, \"MAE\": mae_score, \"R2\": r2_score}\n",
    "    print(f\"\\tMétricas de error:\\n\\t\\tRMSE: {rmse_score}\\n\\t\\tMAE: {mae_score}\\n\\t\\tR2: {r2_score}\")\n",
    "\n",
    "    # path = os.path.join(root_path, f\"Modelos/entrada_amperaje_multi/Inversor_{inv_id - 20}/Repositorio/{model_name}-{pd.Timestamp.now()}/\")\n",
    "    # os.makedirs(path)\n",
    "    # with open(path+'model.model', \"wb\") as archivo_salida:\n",
    "    #     pickle.dump(pipeline_model, archivo_salida)\n",
    "    # with open(path+'informe_modelo.json', 'w') as archivo_json:\n",
    "    #     informe = {\"promediado\": promediado,\n",
    "    #                 \"intervalo_min\": intervalo,\n",
    "    #                 \"normalizacion\": normalizacion,\n",
    "    #                 \"optimizacion\": optimizacion,\n",
    "    #                 \"metricas\": metricas,\n",
    "    #                 \"por_fases\": stage,\n",
    "    #                 \"device\": device,\n",
    "    #                 \"hiperparametros\": pipeline_model.named_steps['regressor'].save_config(),\n",
    "    #                 \"training_input_description\": train_df[perc_attr + std_attr].describe().loc[[\"mean\", \"std\", \"min\", \"max\"]].to_dict(),\n",
    "    #                 \"training_target_description\": train_df[\"potencia_act\"].describe().to_dict()\n",
    "    #                 }\n",
    "    #     json.dump(informe, archivo_json)\n",
    "\n",
    "    # Generación de gráficos: comparativa de valores reales y predichos, histograma de diferencias\n",
    "    y_test_sampled, _,y_pred_sampled, _ = train_test_split(prediction_df[\"amp_dc\"], prediction_df[\"y_pred\"], train_size = 0.25)\n",
    "    plt.figure()\n",
    "    plt.tight_layout()\n",
    "    plt.scatter(y_test_sampled, y_pred_sampled, marker = \".\")\n",
    "    plt.plot([min(y_test_sampled), max(y_test_sampled)], [min(y_test_sampled), max(y_test_sampled)], color='black', linestyle='-', linewidth=1)\n",
    "    plt.xlabel(\"Valores reales\")\n",
    "    plt.ylabel(\"Valores predichos\")\n",
    "    plt.title(\"Comparación de valores reales y predichos\")\n",
    "    # plt.savefig(path + \"scatter_validacion.png\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.tight_layout()\n",
    "    ax = sns.histplot(prediction_df[\"amp_dc\"] - prediction_df[\"y_pred\"], kde=True, stat='percent')\n",
    "    ax.axvline(x=0, color='black', linestyle='--', linewidth = 0.35, label='x=0')\n",
    "    plt.title('Histograma de las diferencias entre valores reales y predichos')\n",
    "    plt.xlabel('Diferencia')\n",
    "    plt.ylabel('Porcentaje')\n",
    "    # plt.savefig(path + \"histogram_validacion.png\")\n",
    "\n",
    "    # Generación de gráficos: comparativa de RMSE y RMSE relativo por entrada\n",
    "    rmse_list = []\n",
    "    rmse_r_list = []\n",
    "    entrada_list = []\n",
    "    for group in prediction_df.groupby([\"dispositivo_id\", \"entrada_id\"]):\n",
    "        entrada_list.append(group[0][1])\n",
    "        rmse_score = round(mse(group[1][\"amp_dc\"], group[1][\"y_pred\"], squared=False), 2)\n",
    "        rmse_r_score = round((mse(group[1][\"amp_dc\"], group[1][\"y_pred\"], squared=False)*100/group[1]['amp_dc'].mean()), 2)\n",
    "        mae_score = round(mae(group[1][\"amp_dc\"], group[1][\"y_pred\"]), 2)\n",
    "        r2_score = round(r2(group[1][\"amp_dc\"], group[1][\"y_pred\"]), 3)\n",
    "        rmse_list.append(rmse_score)\n",
    "        rmse_r_list.append(rmse_r_score)\n",
    "\n",
    "        metricas_entrada = {\"RMSE\": rmse_score, \"RMSE %\": rmse_r_score, \"MAE\": mae_score, \"R2\": r2_score}\n",
    "        \n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Entrada')\n",
    "    ax1.set_ylabel('RMSE', color=color)\n",
    "    ax1.plot(entrada_list, rmse_list, color=color, linewidth=1)\n",
    "    sns.scatterplot(x=entrada_list, y=rmse_list, color=color, ax=ax1)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('RMSE Relativo', color=color)\n",
    "    if rmse_r_score.max() - rmse_r_score.min() > 0.25:\n",
    "        ax2.set_yscale('log')\n",
    "    ax2.plot(entrada_list, rmse_r_list, color=color, linewidth=1)\n",
    "    sns.scatterplot(x=entrada_list, y=rmse_r_list, color=color, ax=ax2)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    plt.title(f'RMSE por entrada para el inversor {inv_id - 20}')\n",
    "    plt.xticks(entrada_list)\n",
    "    plt.tight_layout()\n",
    "    ax1.grid(True, which='major', color='gray', linewidth=0.5)\n",
    "    #ax2.grid(True, which='minor', color='gray', linewidth=0.5)\n",
    "    # plt.savefig(path + \"rmse_entrada.png\")\n",
    "\n",
    "    # Generación de gráficos: comparativa de RMSE y RMSE relativo por hora\n",
    "    rmse_list = []\n",
    "    rmse_r_list = []\n",
    "    hora_list = []\n",
    "    for group in prediction_df.groupby([\"dispositivo_id\", prediction_df[\"datetime_utc\"].dt.hour]):\n",
    "        hora_list.append(group[0][1])\n",
    "        rmse_score = round(mse(group[1][\"amp_dc\"], group[1][\"y_pred\"], squared=False), 2)\n",
    "        rmse_r_score = round((mse(group[1][\"amp_dc\"], group[1][\"y_pred\"], squared=False)*100/group[1]['amp_dc'].mean()), 2)\n",
    "        mae_score = round(mae(group[1][\"amp_dc\"], group[1][\"y_pred\"]), 2)\n",
    "        r2_score = round(r2(group[1][\"amp_dc\"], group[1][\"y_pred\"]), 3)\n",
    "        rmse_list.append(rmse_score)\n",
    "        rmse_r_list.append(rmse_r_score)\n",
    "\n",
    "        metricas_entrada = {\"RMSE\": rmse_score, \"RMSE %\": rmse_r_score, \"MAE\": mae_score, \"R2\": r2_score}\n",
    "        \n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Hora')\n",
    "    ax1.set_ylabel('RMSE', color=color)\n",
    "    ax1.plot(hora_list, rmse_list, color=color, linewidth=1)\n",
    "    sns.scatterplot(x=hora_list, y=rmse_list, color=color, ax=ax1)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('RMSE Relativo', color=color)\n",
    "    if rmse_r_score.max() - rmse_r_score.min() > 0.25:\n",
    "        ax2.set_yscale('log')\n",
    "    ax2.plot(hora_list, rmse_r_list, color=color, linewidth=1)\n",
    "    sns.scatterplot(x=hora_list, y=rmse_r_list, color=color, ax=ax2)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    plt.title(f'Comparativa de RMSE y RMSE Relativo por hora para el inversor {inv_id - 20}')\n",
    "    plt.xticks(hora_list)\n",
    "    plt.tight_layout()\n",
    "    ax1.grid(True, which='major', color='gray', linewidth=0.5)\n",
    "    ax2.grid(True, which='minor', color='gray', linewidth=0.5)\n",
    "    # plt.savefig(path + \"rmse_hora.png\")\n",
    "    plt.close(\"all\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df[(prediction_df[\"dispositivo_id\"] == 21) & (np.abs(prediction_df[\"amp_dc\"] - prediction_df[\"y_pred\"]) > 3*11.17) & (prediction_df[\"amp_dc\"] < 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df[(disp_df.index.get_level_values(\"dispositivo_id\") == 21) & (disp_df.index.get_level_values(\"datetime_utc\") == \"2022-07-18 11:55:00+00:00\")].filter(like=\"amp_dc\").values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df[(disp_df.index.get_level_values(\"dispositivo_id\") == 21) & (disp_df.index.get_level_values(\"datetime_utc\") == \"2022-07-18 11:55:00+00:00\")].filter(like=\"amp_dc\").values.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df[(disp_df.index.get_level_values(\"dispositivo_id\") == 21) & (disp_df.index.get_level_values(\"datetime_utc\") == \"2022-07-18 11:55:00+00:00\")].filter(like=\"amp_dc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df[(disp_df.index.get_level_values(\"dispositivo_id\") == 21) & (disp_df.index.get_level_values(\"datetime_utc\") == \"2022-07-18 11:55:00+00:00\")].iloc[:, 15:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Test_FVPredictive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
