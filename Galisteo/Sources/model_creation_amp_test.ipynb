{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuración para el entrenamiento de modelos de predicción de entrada de corriente continua en inversores fotovoltaicos:\n",
      "\n",
      "\n",
      "Las opciones seleccionadas son: \n",
      "Modelo: XGBRegressor       \n",
      "Promediado: 30 min       \n",
      "Normalización: True       \n",
      "Optimización: False       \n",
      "Entrenamiento: cpu\n",
      "\n",
      "Conexión a la base de datos fvpredictive (esquema galisteo) establecida\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sqlalchemy import create_engine\n",
    "import warnings\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials, space_eval\n",
    "from functools import partial\n",
    "from sklearn.metrics import mean_squared_error as mse, \\\n",
    "                            mean_absolute_error as mae, \\\n",
    "                            r2_score as r2\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "def discriminador(row):\n",
    "    mean = np.mean(row)\n",
    "    std = np.std(row)\n",
    "    threshold = 3 * std\n",
    "    outlier = np.abs(row - mean) > threshold\n",
    "    return outlier\n",
    "\n",
    "class MultiOutputOpt(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_class=XGBRegressor, device=\"cpu\", trials=None):\n",
    "        self.model_class = model_class\n",
    "        self.device = device\n",
    "        self.models = {}  \n",
    "        self.trials = trials if trials is not None else {}\n",
    "        try:\n",
    "            print(len(self.trials[0].trials))\n",
    "        except:\n",
    "            pass\n",
    "        self.params = {}\n",
    "        \n",
    "    def objective(self, space_params, train_set, cv_folds):\n",
    "        n_estimators = int(space_params.pop('n_estimators'))\n",
    "        params = {'device': self.device,\n",
    "            'objective': 'reg:squarederror', \n",
    "            'tree_method': \"hist\", \n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1}\n",
    "        params = {**params, **space_params}\n",
    "        if 'max_depth' in params:\n",
    "            params['max_depth'] = int(params['max_depth'])\n",
    "        if 'min_child_weight' in params:\n",
    "            params['min_child_weight'] = max(1, int(params['min_child_weight']))\n",
    "        print\n",
    "        cv_result = xgb.cv(params, train_set, nfold = cv_folds, num_boost_round = n_estimators, early_stopping_rounds = 100, metrics = 'rmse', as_pandas = True)\n",
    "        score = cv_result['test-rmse-mean'].min()\n",
    "        return {'loss': score, 'status': STATUS_OK}\n",
    "        \n",
    "    def optimize(self, X, y, space, cv_folds, gamma_algo = 1, STALL_LIMIT = 5, MAX_EVALS_PER_RUN = 250):\n",
    "        for col in tqdm(range(y.shape[1]), total = y.shape[1]):\n",
    "            if self.params.get(col) is not None:\n",
    "                space.update(self.params[col])\n",
    "            train_set = xgb.DMatrix(X, label=y[:, col])\n",
    "            trials = Trials()\n",
    "            TOTAL_EVALS = len(trials.trials)\n",
    "            STALL_LIMIT = STALL_LIMIT\n",
    "            MAX_EVALS_PER_RUN = MAX_EVALS_PER_RUN\n",
    "            best_loss = np.inf\n",
    "            stall_counter = 0\n",
    "            num_evals = TOTAL_EVALS\n",
    "            run_counter = 0\n",
    "            upper_limit = (MAX_EVALS_PER_RUN * (STALL_LIMIT - 1)) * 10\n",
    "            while stall_counter < STALL_LIMIT and num_evals < TOTAL_EVALS + upper_limit:\n",
    "                best = fmin(fn=lambda space: self.objective(space, train_set = train_set, cv_folds = cv_folds), \n",
    "                        space = space, \n",
    "                        algo = partial(tpe.suggest, gamma = gamma_algo),\n",
    "                        max_evals = num_evals + MAX_EVALS_PER_RUN, \n",
    "                        trials = trials,\n",
    "                        verbose = False)  # Cambiado a True para que devuelva los parámetros óptimos\n",
    "                best_params = space_eval(space, best)  # Obtener los parámetros óptimos en su forma original\n",
    "                new_loss = trials.best_trial['result']['loss']\n",
    "                if new_loss < best_loss:\n",
    "                    threshold = 0.001\n",
    "                    if abs(new_loss - best_loss) <= threshold:\n",
    "                        stall_counter += 1\n",
    "                    else:\n",
    "                        stall_counter = 0\n",
    "                    best_loss = new_loss\n",
    "                else:\n",
    "                    stall_counter += 1\n",
    "                num_evals += MAX_EVALS_PER_RUN\n",
    "                run_counter += 1\n",
    "                gamma_algo -= 0.05\n",
    "            print(f\"\\tEntrenamiento para entrada {col+1} finalizado\")\n",
    "            print(f\"\\tNúmero de evaluaciones realizadas: {num_evals}\")\n",
    "            print(f\"\\tBest params: {best_params}\")\n",
    "            print(f\"\\tBest loss: {best_loss}\")\n",
    "\n",
    "            final_params = {\"device\": self.device,\n",
    "                        \"objective\": 'reg:squarederror', \n",
    "                        \"tree_method\": \"hist\",\n",
    "                        \"n_jobs\": -1}\n",
    "            final_params = {**final_params, **best_params}\n",
    "            final_params.pop('n_estimators')\n",
    "            if 'max_depth' in final_params:\n",
    "                final_params['max_depth'] = int(final_params['max_depth'])\n",
    "            if 'min_child_weight' in final_params:\n",
    "                final_params['min_child_weight'] = max(1, int(final_params['min_child_weight']))\n",
    "            model = xgb.train(final_params, train_set, num_boost_round = int(best_params['n_estimators']))\n",
    "            self.models[col] = model\n",
    "            self.trials[col] = trials\n",
    "            self.params[col] = best_params  # Guardar los parámetros óptimos en su forma original\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        dmatrix = xgb.DMatrix(X)\n",
    "        for col, model in self.models.items():\n",
    "            pred = model.predict(dmatrix)\n",
    "            predictions.append(pred)\n",
    "        return np.column_stack(predictions)\n",
    "        \n",
    "    \n",
    "print(\"Configuración para el entrenamiento de modelos de predicción de entrada de corriente continua en inversores fotovoltaicos:\", end=\"\\n\\n\")\n",
    "# valid_models = {\"1\": \"XGBRegressor\", \"2\": \"RandomForestRegressor\"}\n",
    "# model_name = \"\"\n",
    "# while model_name not in valid_models.keys():\n",
    "#     model_name = input(\"Ingrese el valor numérico para el tipo de modelo que desea utilizar (XGBRegressor[1], RandomForestRegressor[2]): \")\n",
    "# model_name = valid_models[model_name]\n",
    "model_name = \"XGBRegressor\"\n",
    "\n",
    "valid_responses = [\"y\", \"n\"]\n",
    "\n",
    "promediado = \"\"\n",
    "intervalo = 0\n",
    "intervalos = [5, 10, 15, 20, 30, 60]\n",
    "while promediado not in valid_responses:\n",
    "    promediado = input(\"¿Desea promediar los datos de entrada? (Y/N): \").lower()\n",
    "promediado = promediado == \"y\"\n",
    "if promediado:\n",
    "    while intervalo not in intervalos:\n",
    "        intervalo = int(input(\"\\tIngrese el intervalo de tiempo en minutos [5, 10, 15, 20, 30, 60] para el promediado de los datos de entrada: \"))\n",
    "\n",
    "normalizacion = \"\"\n",
    "while normalizacion not in valid_responses:\n",
    "    normalizacion = input(\"¿Desea normalizar el target? (Y/N): \").lower()\n",
    "normalizacion = normalizacion == \"y\"\n",
    "\n",
    "optimizacion = \"\"\n",
    "while optimizacion not in valid_responses:\n",
    "    optimizacion = input(\"¿Desea optimizar el modelo? (Y/N): \").lower()\n",
    "optimizacion = optimizacion == \"y\"\n",
    "\n",
    "stage = True\n",
    "valid_devices = [\"cpu\", \"cuda\"]\n",
    "device = \"\"\n",
    "if optimizacion:\n",
    "    while stage not in valid_responses:\n",
    "        stage = input(\"\\t¿Desea optimizar por fases? (Y/N): \").lower()\n",
    "    stage = stage == \"y\"\n",
    "    # Comprobación de la disponibilidad de GPU para el entrenamiento\n",
    "    if torch.cuda.is_available():\n",
    "        while device not in valid_devices:\n",
    "            device_bool = input(\"¿Desea utilizar GPU para el entrenamiento? (Y/N): \").lower()\n",
    "            if device_bool == \"y\":\n",
    "                device = \"cuda\"\n",
    "            else:\n",
    "                device = \"cpu\"\n",
    "else:\n",
    "    stage = False\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"\\nLas opciones seleccionadas son: \\nModelo: {model_name} \\\n",
    "      \\nPromediado: {intervalo} min \\\n",
    "      \\nNormalización: {normalizacion} \\\n",
    "      \\nOptimización: {optimizacion} \\\n",
    "      \\nEntrenamiento: {device}\", end=\"\\n\\n\")\n",
    "\n",
    "root_path = \"/home/upo/Desktop/Test_FVPredictive/FVPredictive_TEST/Galisteo\"\n",
    "params = None\n",
    "for filename in os.listdir(root_path):\n",
    "    if \"params.json\" in filename:\n",
    "        with open(os.path.join(root_path, filename)) as f:\n",
    "            params = json.load(f)\n",
    "if params is None:\n",
    "    print(\"No se ha encontrado el archivo de parámetros para la conexión a la base de datos\")\n",
    "    sys.exit()\n",
    "data_path = os.path.join(root_path, params[\"data_path\"])\n",
    "schema_name = params[\"schema\"]\n",
    "\n",
    "password = params['password'].replace('@', '%40')\n",
    "engine = create_engine(f'postgresql://{params[\"user\"]}:{password}@{params[\"host\"]}:{params[\"port\"]}/{params[\"dbname\"]}')\n",
    "print(f\"Conexión a la base de datos {params['dbname']} (esquema {schema_name}) establecida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_mod_string = 30\n",
    "sup_mod = 2\n",
    "# Carga de los datos de entrenamiento\n",
    "if promediado:\n",
    "    main_query = f\"\"\"\n",
    "            WITH inv AS (\n",
    "                SELECT \n",
    "                    date_trunc('hour', datetime_utc) + \n",
    "                    INTERVAL '{intervalo} min' * floor(date_part('minute', datetime_utc) / {intervalo}) as datetime_utc_rounded,\n",
    "                    dispositivo_id,\n",
    "                    MIN(inv.id) AS id, \n",
    "                    AVG(potencia_act) as potencia_act, \n",
    "                    AVG(amp_dc) as amp_dc,\n",
    "                    det.entrada_id\n",
    "                FROM {schema_name}.inversores AS inv\n",
    "                JOIN {schema_name}.inversores_detalle AS det\n",
    "                    ON inv.id = det.id\n",
    "                WHERE (dispositivo_id != 26)\n",
    "                    AND (alarma = 0)\n",
    "                    AND (estado = 6)\n",
    "                GROUP BY datetime_utc_rounded, dispositivo_id, det.entrada_id \n",
    "            ),\n",
    "            met AS (\n",
    "                SELECT \n",
    "                    date_trunc('hour', datetime_utc) + \n",
    "                    INTERVAL '{intervalo} min' * floor(date_part('minute', datetime_utc) / {intervalo}) as datetime_utc_rounded, \n",
    "                    dispositivo_id,\n",
    "                    AVG(rad_poa) AS rad_poa, \n",
    "                    AVG(rad_hor) AS rad_hor, \n",
    "                    AVG(rad_celda1) AS rad_celda1,\n",
    "                    AVG(rad_celda2) AS rad_celda2, \n",
    "                    AVG(temp_amb) AS temp_amb, \n",
    "                    AVG(temp_panel1) AS temp_panel1,\n",
    "                    AVG(temp_panel2) AS temp_panel2, \n",
    "                    AVG(cloud_impact) AS cloud_impact,\n",
    "                    BOOL_OR(daylight) AS daylight\n",
    "                FROM {schema_name}.meteo\n",
    "                    WHERE daylight = true\n",
    "                GROUP BY dispositivo_id, datetime_utc_rounded\n",
    "            )\n",
    "            SELECT \n",
    "                inv.id,\n",
    "                inv.dispositivo_id,\n",
    "                inv.entrada_id,\n",
    "                inv.datetime_utc_rounded as datetime_utc, \n",
    "                potencia_act,  \n",
    "                num_strings, \n",
    "                rad_poa,\n",
    "                rad_hor, \n",
    "                rad_celda1, \n",
    "                rad_celda2, \n",
    "                temp_amb, \n",
    "                temp_panel1, \n",
    "                temp_panel2,\n",
    "                cloud_impact,\n",
    "                motivo,\n",
    "                consigna_pot_act_planta,\n",
    "                amp_dc\n",
    "            FROM inv\n",
    "            JOIN {schema_name}.distrib_inversores dist\n",
    "                ON dist.dispositivo_id = inv.dispositivo_id\n",
    "                    AND dist.entrada_id = inv.entrada_id\n",
    "            JOIN {schema_name}.dispositivos AS disp\n",
    "                ON disp.dispositivo_id = inv.dispositivo_id\n",
    "            JOIN met\n",
    "                ON met.datetime_utc_rounded = inv.datetime_utc_rounded\n",
    "                    AND met.dispositivo_id = disp.meteo_cercana_id\n",
    "            JOIN {schema_name}.ree AS ree\n",
    "                ON ree.datetime_utc = inv.datetime_utc_rounded\n",
    "            ORDER BY 5, 2, 3, 4;\"\"\"\n",
    "else:\n",
    "    main_query = f\"\"\"\n",
    "        WITH f AS (\n",
    "            SELECT *\n",
    "                FROM {schema_name}.inversores\n",
    "                WHERE (EXTRACT(MINUTE FROM datetime_utc) %% 5 = 0)\n",
    "                    AND (EXTRACT(SECOND FROM datetime_utc) = 0)\n",
    "                    AND (EXTRACT(MONTH FROM datetime_utc) != 10)\n",
    "                    AND (alarma = 0)\n",
    "                    AND (estado = 6)\n",
    "                ORDER BY datetime_utc)\n",
    "        SELECT f.id, f.dispositivo_id, det.entrada_id, f.datetime_utc, potencia_act, num_strings, \n",
    "                rad_poa, rad_hor, rad_celda1, rad_celda2, temp_amb, temp_panel1, temp_panel2, cloud_impact, \n",
    "                motivo, consigna_pot_act_planta, amp_dc\n",
    "            FROM f\n",
    "            JOIN {schema_name}.inversores_detalle AS det\n",
    "                ON f.id = det.id\n",
    "            JOIN {schema_name}.distrib_inversores dist\n",
    "                ON  dist.dispositivo_id = f.dispositivo_id\n",
    "                    AND dist.entrada_id = det.entrada_id\n",
    "            JOIN {schema_name}.dispositivos AS disp\n",
    "                ON disp.dispositivo_id = f.dispositivo_id\n",
    "            JOIN {schema_name}.meteo AS met\n",
    "                ON met.dispositivo_id = disp.meteo_cercana_id\n",
    "                    AND met.datetime_utc = f.datetime_utc\n",
    "            JOIN {schema_name}.ree AS ree\n",
    "                ON ree.datetime_utc = f.datetime_utc\n",
    "            WHERE daylight = true\n",
    "            ORDER BY 4, 2, 3;\"\"\"\n",
    "    \n",
    "chunksize = 100000\n",
    "chunks = pd.read_sql_query(main_query, engine, chunksize=chunksize)\n",
    "main_df = pd.DataFrame()\n",
    "for chunk in chunks:\n",
    "    main_df = pd.concat([main_df, chunk], ignore_index = True)\n",
    "del chunks, chunk\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dispositivo_id</th>\n",
       "      <th>entrada_id</th>\n",
       "      <th>datetime_utc</th>\n",
       "      <th>potencia_act</th>\n",
       "      <th>num_strings</th>\n",
       "      <th>rad_poa</th>\n",
       "      <th>rad_hor</th>\n",
       "      <th>rad_celda1</th>\n",
       "      <th>rad_celda2</th>\n",
       "      <th>temp_amb</th>\n",
       "      <th>temp_panel1</th>\n",
       "      <th>temp_panel2</th>\n",
       "      <th>cloud_impact</th>\n",
       "      <th>motivo</th>\n",
       "      <th>consigna_pot_act_planta</th>\n",
       "      <th>amp_dc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3329739</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-06-29 07:00:00+02:00</td>\n",
       "      <td>-16.000000</td>\n",
       "      <td>24</td>\n",
       "      <td>23.631667</td>\n",
       "      <td>20.818333</td>\n",
       "      <td>24.118333</td>\n",
       "      <td>23.506667</td>\n",
       "      <td>19.136667</td>\n",
       "      <td>17.655</td>\n",
       "      <td>16.613333</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>38.7</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3329739</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-06-29 07:00:00+02:00</td>\n",
       "      <td>-16.000000</td>\n",
       "      <td>24</td>\n",
       "      <td>23.631667</td>\n",
       "      <td>20.818333</td>\n",
       "      <td>24.118333</td>\n",
       "      <td>23.506667</td>\n",
       "      <td>19.136667</td>\n",
       "      <td>17.655</td>\n",
       "      <td>16.613333</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>38.7</td>\n",
       "      <td>-0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3329739</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-06-29 07:00:00+02:00</td>\n",
       "      <td>-16.000000</td>\n",
       "      <td>21</td>\n",
       "      <td>23.631667</td>\n",
       "      <td>20.818333</td>\n",
       "      <td>24.118333</td>\n",
       "      <td>23.506667</td>\n",
       "      <td>19.136667</td>\n",
       "      <td>17.655</td>\n",
       "      <td>16.613333</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>38.7</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3329739</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-06-29 07:00:00+02:00</td>\n",
       "      <td>-16.000000</td>\n",
       "      <td>24</td>\n",
       "      <td>23.631667</td>\n",
       "      <td>20.818333</td>\n",
       "      <td>24.118333</td>\n",
       "      <td>23.506667</td>\n",
       "      <td>19.136667</td>\n",
       "      <td>17.655</td>\n",
       "      <td>16.613333</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>38.7</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3329739</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-06-29 07:00:00+02:00</td>\n",
       "      <td>-16.000000</td>\n",
       "      <td>24</td>\n",
       "      <td>23.631667</td>\n",
       "      <td>20.818333</td>\n",
       "      <td>24.118333</td>\n",
       "      <td>23.506667</td>\n",
       "      <td>19.136667</td>\n",
       "      <td>17.655</td>\n",
       "      <td>16.613333</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>38.7</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397035</th>\n",
       "      <td>10864804</td>\n",
       "      <td>32</td>\n",
       "      <td>12</td>\n",
       "      <td>2022-04-19 13:00:00+02:00</td>\n",
       "      <td>3567.183333</td>\n",
       "      <td>24</td>\n",
       "      <td>1149.805001</td>\n",
       "      <td>1111.385004</td>\n",
       "      <td>1118.278336</td>\n",
       "      <td>1105.185003</td>\n",
       "      <td>22.110000</td>\n",
       "      <td>34.135</td>\n",
       "      <td>30.863333</td>\n",
       "      <td>83.421234</td>\n",
       "      <td>0</td>\n",
       "      <td>38.7</td>\n",
       "      <td>192.626667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397036</th>\n",
       "      <td>10864804</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>2022-04-19 13:00:00+02:00</td>\n",
       "      <td>3567.183333</td>\n",
       "      <td>24</td>\n",
       "      <td>1149.805001</td>\n",
       "      <td>1111.385004</td>\n",
       "      <td>1118.278336</td>\n",
       "      <td>1105.185003</td>\n",
       "      <td>22.110000</td>\n",
       "      <td>34.135</td>\n",
       "      <td>30.863333</td>\n",
       "      <td>83.421234</td>\n",
       "      <td>0</td>\n",
       "      <td>38.7</td>\n",
       "      <td>188.751667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397037</th>\n",
       "      <td>10864804</td>\n",
       "      <td>32</td>\n",
       "      <td>14</td>\n",
       "      <td>2022-04-19 13:00:00+02:00</td>\n",
       "      <td>3567.183333</td>\n",
       "      <td>24</td>\n",
       "      <td>1149.805001</td>\n",
       "      <td>1111.385004</td>\n",
       "      <td>1118.278336</td>\n",
       "      <td>1105.185003</td>\n",
       "      <td>22.110000</td>\n",
       "      <td>34.135</td>\n",
       "      <td>30.863333</td>\n",
       "      <td>83.421234</td>\n",
       "      <td>0</td>\n",
       "      <td>38.7</td>\n",
       "      <td>190.211667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397038</th>\n",
       "      <td>10864804</td>\n",
       "      <td>32</td>\n",
       "      <td>15</td>\n",
       "      <td>2022-04-19 13:00:00+02:00</td>\n",
       "      <td>3567.183333</td>\n",
       "      <td>24</td>\n",
       "      <td>1149.805001</td>\n",
       "      <td>1111.385004</td>\n",
       "      <td>1118.278336</td>\n",
       "      <td>1105.185003</td>\n",
       "      <td>22.110000</td>\n",
       "      <td>34.135</td>\n",
       "      <td>30.863333</td>\n",
       "      <td>83.421234</td>\n",
       "      <td>0</td>\n",
       "      <td>38.7</td>\n",
       "      <td>191.395000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397039</th>\n",
       "      <td>10864804</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>2022-04-19 13:00:00+02:00</td>\n",
       "      <td>3567.183333</td>\n",
       "      <td>21</td>\n",
       "      <td>1149.805001</td>\n",
       "      <td>1111.385004</td>\n",
       "      <td>1118.278336</td>\n",
       "      <td>1105.185003</td>\n",
       "      <td>22.110000</td>\n",
       "      <td>34.135</td>\n",
       "      <td>30.863333</td>\n",
       "      <td>83.421234</td>\n",
       "      <td>0</td>\n",
       "      <td>38.7</td>\n",
       "      <td>174.198334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2397040 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id  dispositivo_id  entrada_id               datetime_utc  \\\n",
       "0         3329739              24           1  2023-06-29 07:00:00+02:00   \n",
       "1         3329739              24           2  2023-06-29 07:00:00+02:00   \n",
       "2         3329739              24           3  2023-06-29 07:00:00+02:00   \n",
       "3         3329739              24           4  2023-06-29 07:00:00+02:00   \n",
       "4         3329739              24           5  2023-06-29 07:00:00+02:00   \n",
       "...           ...             ...         ...                        ...   \n",
       "2397035  10864804              32          12  2022-04-19 13:00:00+02:00   \n",
       "2397036  10864804              32          13  2022-04-19 13:00:00+02:00   \n",
       "2397037  10864804              32          14  2022-04-19 13:00:00+02:00   \n",
       "2397038  10864804              32          15  2022-04-19 13:00:00+02:00   \n",
       "2397039  10864804              32          16  2022-04-19 13:00:00+02:00   \n",
       "\n",
       "         potencia_act  num_strings      rad_poa      rad_hor   rad_celda1  \\\n",
       "0          -16.000000           24    23.631667    20.818333    24.118333   \n",
       "1          -16.000000           24    23.631667    20.818333    24.118333   \n",
       "2          -16.000000           21    23.631667    20.818333    24.118333   \n",
       "3          -16.000000           24    23.631667    20.818333    24.118333   \n",
       "4          -16.000000           24    23.631667    20.818333    24.118333   \n",
       "...               ...          ...          ...          ...          ...   \n",
       "2397035   3567.183333           24  1149.805001  1111.385004  1118.278336   \n",
       "2397036   3567.183333           24  1149.805001  1111.385004  1118.278336   \n",
       "2397037   3567.183333           24  1149.805001  1111.385004  1118.278336   \n",
       "2397038   3567.183333           24  1149.805001  1111.385004  1118.278336   \n",
       "2397039   3567.183333           21  1149.805001  1111.385004  1118.278336   \n",
       "\n",
       "          rad_celda2   temp_amb  temp_panel1  temp_panel2  cloud_impact  \\\n",
       "0          23.506667  19.136667       17.655    16.613333    100.000000   \n",
       "1          23.506667  19.136667       17.655    16.613333    100.000000   \n",
       "2          23.506667  19.136667       17.655    16.613333    100.000000   \n",
       "3          23.506667  19.136667       17.655    16.613333    100.000000   \n",
       "4          23.506667  19.136667       17.655    16.613333    100.000000   \n",
       "...              ...        ...          ...          ...           ...   \n",
       "2397035  1105.185003  22.110000       34.135    30.863333     83.421234   \n",
       "2397036  1105.185003  22.110000       34.135    30.863333     83.421234   \n",
       "2397037  1105.185003  22.110000       34.135    30.863333     83.421234   \n",
       "2397038  1105.185003  22.110000       34.135    30.863333     83.421234   \n",
       "2397039  1105.185003  22.110000       34.135    30.863333     83.421234   \n",
       "\n",
       "         motivo  consigna_pot_act_planta      amp_dc  \n",
       "0             0                     38.7    0.400000  \n",
       "1             0                     38.7   -0.200000  \n",
       "2             0                     38.7    0.400000  \n",
       "3             0                     38.7    2.500000  \n",
       "4             0                     38.7    2.500000  \n",
       "...         ...                      ...         ...  \n",
       "2397035       0                     38.7  192.626667  \n",
       "2397036       0                     38.7  188.751667  \n",
       "2397037       0                     38.7  190.211667  \n",
       "2397038       0                     38.7  191.395000  \n",
       "2397039       0                     38.7  174.198334  \n",
       "\n",
       "[2397040 rows x 17 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carga inicial de 2397040 registros\n",
      "Entrada de corriente continua normalizada según el número de strings\n"
     ]
    }
   ],
   "source": [
    "# Normalización de la entrada de corriente continua, formateo de fechas y escalado de potencia\n",
    "print(f\"Carga inicial de {main_df.shape[0]} registros\")\n",
    "if normalizacion:\n",
    "    print(\"Entrada de corriente continua normalizada según el número de strings\")\n",
    "    main_df[\"amp_dc\"] = main_df[\"amp_dc\"]/main_df[\"num_strings\"]\n",
    "else:\n",
    "    print(\"Entrada de corriente continua sin normalizar\")\n",
    "main_df[\"datetime_utc\"] = pd.to_datetime(main_df[\"datetime_utc\"], utc = True)\n",
    "main_df[\"potencia_act\"] = main_df[\"potencia_act\"] * 1000\n",
    "main_df = main_df.sort_values(by = [\"datetime_utc\", \"dispositivo_id\", \"entrada_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de registros del dataframe tras pivotar: 149815\n"
     ]
    }
   ],
   "source": [
    "# Pivotado de las entradas de corriente continua\n",
    "target_df = main_df.pivot(index=[\"dispositivo_id\", \"datetime_utc\"], columns='entrada_id', values='amp_dc')\n",
    "target_df.columns = [\"amp_dc_\" + str(col) for col in target_df.columns]\n",
    "print(f\"Número de registros del dataframe tras pivotar: {target_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros descartados por corrientes anómalas ingresando en el inversor: 25125\n"
     ]
    }
   ],
   "source": [
    "# Descarte de registros con corriente anómala\n",
    "target_df[\"outlier\"] = target_df.apply(discriminador, axis=1).any(axis=1)\n",
    "n_corriente_outlier = target_df[target_df[\"outlier\"]].shape[0]\n",
    "target_df = target_df[~target_df[\"outlier\"]].drop(columns=\"outlier\")\n",
    "print(f\"Registros descartados por corrientes anómalas ingresando en el inversor: {n_corriente_outlier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros descartados por corriente negativa: 179\n"
     ]
    }
   ],
   "source": [
    "# Descarte de registros con corriente negativa\n",
    "q1 = main_df[main_df['amp_dc'] < 0]['amp_dc'].quantile(0.25)\n",
    "q3 = main_df[main_df['amp_dc'] < 0]['amp_dc'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "n_corriente_neg = target_df[target_df < lower_bound].dropna(how='all').shape[0]\n",
    "target_df = target_df[target_df >= lower_bound].dropna(how='any')\n",
    "target_df[(target_df >= lower_bound) & (target_df <= 0)] = 0\n",
    "main_df = main_df.drop(columns=[\"entrada_id\", \"amp_dc\"]).drop_duplicates(subset=[\"id\", \"datetime_utc\"]).set_index([\"dispositivo_id\", \"datetime_utc\"])\n",
    "main_df = main_df.merge(target_df, left_index=True, right_index=True, how=\"inner\").sort_index()\n",
    "del target_df\n",
    "gc.collect()\n",
    "print(f\"Registros descartados por corriente negativa: {n_corriente_neg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Búsqueda de outliers basándose en la potencia activa y la potencia solar\n",
    "# num_strings_inv = f\"\"\"SELECT dispositivo_id, SUM(num_strings) as num_strings \n",
    "#                         FROM {schema_name}.distrib_inversores\n",
    "#                         GROUP BY dispositivo_id;\"\"\"\n",
    "# num_strings_inv = pd.read_sql_query(num_strings_inv, engine).sort_values(by=\"dispositivo_id\")\n",
    "# potencia_df = pd.merge(main_df.reset_index()[[\"dispositivo_id\", \"datetime_utc\", \"potencia_act\", \"rad_poa\"]], num_strings_inv, on=\"dispositivo_id\").set_index([\"dispositivo_id\", \"datetime_utc\"])\n",
    "# potencia_df[\"potencia_solar\"] = potencia_df[\"rad_poa\"] * potencia_df[\"num_strings\"] * num_mod_string * sup_mod\n",
    "# potencia_df[\"outlier_solar\"] = np.where(potencia_df[\"potencia_act\"] > 0.20 * potencia_df[\"potencia_solar\"], True, False)\n",
    "# main_df = main_df.merge(potencia_df[[\"outlier_solar\"]], left_index=True, right_index=True, how=\"inner\")\n",
    "# print(f\"Registros descartados por outlier de potencia: {main_df[main_df['outlier_solar'] == True].shape[0]}\")\n",
    "# main_df = main_df[main_df[\"outlier_solar\"] == False].drop(columns = [\"outlier_solar\"])\n",
    "# del potencia_df, num_strings_inv\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Descarte de registros con potencia activa negativa\n",
    "# n_potencia_neg = main_df[main_df[\"potencia_act\"] < 0].shape[0]\n",
    "# main_df = main_df[main_df[\"potencia_act\"] >= 0]\n",
    "# print(f\"Registros descartados por potencia activa negativa: {n_potencia_neg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulación de variables meteorológicas\n",
    "main_df[\"rad_diff\"] = (main_df[\"rad_celda1\"] - main_df[\"rad_celda2\"])\n",
    "main_df[\"temp_panel\"] = (main_df[\"temp_panel1\"] + main_df[\"temp_panel2\"]) / 2\n",
    "main_df = main_df.drop(columns = [\"rad_celda1\", \"rad_celda2\", \"temp_panel1\", \"temp_panel2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulación de variables de consigna\n",
    "main_df[\"motivo\"] = main_df[\"motivo\"].apply(lambda x: 0 if x == 0 else (2 if x == 7 else 1))\n",
    "main_query = f\"\"\"\n",
    "    SELECT MAX(consigna_pot_act_ree)\n",
    "        FROM {schema_name}.ree AS ree;\"\"\"\n",
    "max_pot_act = pd.read_sql_query(main_query, engine).values[0][0]\n",
    "main_df[\"consigna_pot_act_planta\"] = main_df[\"consigna_pot_act_planta\"] / max_pot_act \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignación de variables temporales\n",
    "main_df[\"dia_año\"] = main_df.index.get_level_values(\"datetime_utc\").dayofyear\n",
    "main_df[\"dia_año_sen\"] = np.sin(main_df[\"dia_año\"] * (2*np.pi/365))\n",
    "main_df[\"hora_seg\"] = main_df.index.get_level_values(\"datetime_utc\").hour * 3600 + \\\n",
    "                        main_df.index.get_level_values(\"datetime_utc\").minute * 60 + \\\n",
    "                        main_df.index.get_level_values(\"datetime_utc\").second\n",
    "main_df[\"hora_seg_sen\"] = np.sin(main_df[\"hora_seg\"] * (2*np.pi/86400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado de variables porcentuales\n",
    "main_df[['cloud_impact']] = main_df[['cloud_impact']].apply(lambda x: x/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_power_model(initial_path):\n",
    "    models = {}\n",
    "    for inv_dir in os.listdir(initial_path):\n",
    "        if (\"inversor\" in inv_dir.lower()) & (os.path.isdir(os.path.join(initial_path, inv_dir))):\n",
    "            explotacion_path = os.path.join(initial_path, inv_dir, \"Explotacion\")\n",
    "            for model_dir in os.listdir(explotacion_path):\n",
    "                model_path = os.path.join(explotacion_path, model_dir)\n",
    "                if os.path.isdir(model_path):\n",
    "                    model_dict = {}\n",
    "                    with open(os.path.join(model_path, \"model.model\"), \"rb\") as f:\n",
    "                        model_dict[\"model\"] = pickle.load(f)\n",
    "                    with open(os.path.join(model_path, \"informe_modelo.json\"), \"r\") as f:\n",
    "                        informe_modelo = json.load(f)\n",
    "                        if \"intervalo_min\" in informe_modelo:\n",
    "                            model_dict[\"intervalo\"] = informe_modelo[\"intervalo_min\"]\n",
    "                        else:\n",
    "                            model_dict[\"intervalo\"] = None\n",
    "                        model_dict[\"normalizacion\"] = informe_modelo[\"normalizacion\"]\n",
    "                        model_dict[\"RMSE\"] = informe_modelo[\"metricas\"][\"RMSE\"]\n",
    "                        model_dict[\"RMSE_horas\"] = informe_modelo[\"metricas_hora\"]\n",
    "                    models[inv_dir] = model_dict\n",
    "    return models    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo 21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 202670.5,  641930. , 1126654.6, ..., 1454047.6,  631310.9,\n",
       "        170186.5], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_models = get_power_model(os.path.join(root_path, \"Modelos\", \"potencia_inversor\"))\n",
    "\n",
    "for inv_id in [21]:\n",
    "    print(f\"Dispositivo {inv_id}\")\n",
    "    with open(os.path.join(\"/home/upo/Desktop/Test_FVPredictive/FVPredictive_TEST/Galisteo/Modelos/potencia_inversor/Inversor_1/Repositorio/Booster-2024-02-13 11:46:16.152881\", \"model.model\"), \"rb\") as f:\n",
    "        power_model = pickle.load(f)\n",
    "    disp_df = main_df[main_df.index.get_level_values(\"dispositivo_id\") == inv_id].copy()\n",
    "    disp_df = disp_df.dropna()\n",
    "    power_df = disp_df[[\"rad_poa\", \"rad_hor\", \"rad_diff\", \"temp_amb\", \"temp_panel\", \"cloud_impact\", \"motivo\", \"consigna_pot_act_planta\", \"dia_año_sen\", \"hora_seg_sen\"]]\n",
    "    power_prep_df = power_model.named_steps[\"preprocessor\"].transform(power_df)\n",
    "    dtrain = xgb.DMatrix(power_prep_df)\n",
    "power_model.named_steps[\"regressor\"].predict(xgb.DMatrix(power_prep_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Inversor_1': {'model': Pipeline(steps=[('preprocessor',\n",
       "                   ColumnTransformer(remainder='passthrough',\n",
       "                                     transformers=[('cat',\n",
       "                                                    Pipeline(steps=[('onehot',\n",
       "                                                                     OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                    ['motivo']),\n",
       "                                                   ('std',\n",
       "                                                    Pipeline(steps=[('std_scaler',\n",
       "                                                                     StandardScaler())]),\n",
       "                                                    ['rad_poa', 'rad_hor',\n",
       "                                                     'temp_amb', 'rad_diff',\n",
       "                                                     'temp_panel'])])),\n",
       "                  ('regressor', <xgboost.core.Booster object at 0x7f158f3318a0>)]),\n",
       "  'intervalo': 15,\n",
       "  'normalizacion': True,\n",
       "  'RMSE': 103891.098},\n",
       " 'Inversor_2': {'model': Pipeline(steps=[('preprocessor',\n",
       "                   ColumnTransformer(remainder='passthrough',\n",
       "                                     transformers=[('cat',\n",
       "                                                    Pipeline(steps=[('onehot',\n",
       "                                                                     OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                    ['motivo']),\n",
       "                                                   ('std',\n",
       "                                                    Pipeline(steps=[('std_scaler',\n",
       "                                                                     StandardScaler())]),\n",
       "                                                    ['rad_poa', 'rad_hor',\n",
       "                                                     'temp_amb', 'rad_diff',\n",
       "                                                     'temp_panel'])])),\n",
       "                  ('regressor', <xgboost.core.Booster object at 0x7f15a31b9450>)]),\n",
       "  'intervalo': 15,\n",
       "  'normalizacion': True,\n",
       "  'RMSE': 92440.808}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "power_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo 21\n"
     ]
    }
   ],
   "source": [
    "#for inv_id in np.sort(main_df.index.get_level_values(\"dispositivo_id\").unique()):\n",
    "power_models = get_power_model(os.path.join(root_path, \"Modelos\", \"potencia_inversor\"))\n",
    "for inv_id in [21]:\n",
    "    print(f\"Dispositivo {inv_id}\")\n",
    "    power_model = power_models[f\"Inversor_{inv_id - 20}\"][\"model\"]\n",
    "    disp_df = main_df[main_df.index.get_level_values(\"dispositivo_id\") == inv_id].copy()\n",
    "    disp_df = disp_df.dropna()\n",
    "\n",
    "    # Descarte de valores anómalos en la potencia activa de salida mediante modelo de regresión\n",
    "    power_df = disp_df[[\"rad_poa\", \"rad_hor\", \"rad_diff\", \"temp_amb\", \"temp_panel\", \"cloud_impact\", \"motivo\", \"consigna_pot_act_planta\", \"dia_año_sen\", \"hora_seg_sen\"]]\n",
    "    power_prep_df = power_model.named_steps[\"preprocessor\"].transform(power_df)\n",
    "    dtrain = xgb.DMatrix(power_prep_df)\n",
    "    disp_df[\"potencia_act_pred\"] = power_model.named_steps[\"regressor\"].predict(xgb.DMatrix(power_prep_df))\n",
    "    disp_df[\"outlier_pot\"] = np.where(np.abs(disp_df[\"potencia_act\"] - disp_df[\"potencia_act_pred\"]) > power_models[f\"Inversor_{inv_id - 20}\"][\"RMSE\"], True, False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo 21\n",
      "Registros descartados por outlier de potencia: 21\n",
      "\tRegistros de entrenamiento: 13130\n",
      "\tRegistros de validación: 3283\n",
      "\tMétricas de error:\n",
      "\t\tRMSE: 4.48\n",
      "\t\tMAE: 3.45\n",
      "\t\tR2: 0.995\n",
      "Dispositivo 22\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "columns are missing: {'num_strings'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Descarte de valores anómalos en la potencia activa de salida mediante modelo de regresión\u001b[39;00m\n\u001b[1;32m      9\u001b[0m power_df \u001b[38;5;241m=\u001b[39m disp_df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrad_poa\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrad_hor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrad_diff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp_amb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp_panel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcloud_impact\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmotivo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsigna_pot_act_planta\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdia_año_sen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhora_seg_sen\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m---> 10\u001b[0m power_prep_df \u001b[38;5;241m=\u001b[39m \u001b[43mpower_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreprocessor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpower_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m dtrain \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(power_prep_df)\n\u001b[1;32m     12\u001b[0m disp_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpotencia_act_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m power_model\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregressor\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(xgb\u001b[38;5;241m.\u001b[39mDMatrix(power_prep_df))\n",
      "File \u001b[0;32m~/Desktop/Test_FVPredictive/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/Test_FVPredictive/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py:810\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    808\u001b[0m     diff \u001b[38;5;241m=\u001b[39m all_names \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(X\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m diff:\n\u001b[0;32m--> 810\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns are missing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;66;03m# ndarray was used for fitting or transforming, thus we only\u001b[39;00m\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;66;03m# check that n_features_in_ is consistent\u001b[39;00m\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: columns are missing: {'num_strings'}"
     ]
    }
   ],
   "source": [
    "power_models = get_power_model(os.path.join(root_path, \"Modelos\", \"potencia_inversor\"))\n",
    "for inv_id in np.sort(main_df.index.get_level_values(\"dispositivo_id\").unique()):\n",
    "    print(f\"Dispositivo {inv_id}\")\n",
    "    power_model = power_models[f\"Inversor_{inv_id - 20}\"][\"model\"]\n",
    "    disp_df = main_df[main_df.index.get_level_values(\"dispositivo_id\") == inv_id].copy()\n",
    "    disp_df = disp_df.dropna()\n",
    "\n",
    "    # Descarte de valores anómalos en la potencia activa de salida mediante modelo de regresión\n",
    "    power_df = disp_df[[\"rad_poa\", \"rad_hor\", \"rad_diff\", \"temp_amb\", \"temp_panel\", \"cloud_impact\", \"motivo\", \"consigna_pot_act_planta\", \"dia_año_sen\", \"hora_seg_sen\"]]\n",
    "    power_prep_df = power_model.named_steps[\"preprocessor\"].transform(power_df)\n",
    "    dtrain = xgb.DMatrix(power_prep_df)\n",
    "    disp_df[\"potencia_act_pred\"] = power_model.named_steps[\"regressor\"].predict(xgb.DMatrix(power_prep_df))\n",
    "    disp_df[\"outlier_pot\"] = np.where(np.abs(disp_df[\"potencia_act\"] - disp_df[\"potencia_act_pred\"]) > 1 * power_models[f\"Inversor_{inv_id - 20}\"][\"RMSE\"], True, False)\n",
    "\n",
    "    disp_df = disp_df[disp_df[\"outlier_pot\"] == False].drop(columns = [\"outlier_pot\", \"potencia_act_pred\"])\n",
    "\n",
    "    # Búsqueda de outliers basándose en la potencia activa y la potencia solar\n",
    "    num_strings_inv = f\"\"\"SELECT dispositivo_id, SUM(num_strings) as num_strings \n",
    "                            FROM {schema_name}.distrib_inversores\n",
    "                            GROUP BY dispositivo_id\n",
    "                            HAVING dispositivo_id = {inv_id};\"\"\"\n",
    "    num_strings_inv = pd.read_sql_query(num_strings_inv, engine).sort_values(by=\"dispositivo_id\")\n",
    "    potencia_df = pd.merge(disp_df.reset_index()[[\"dispositivo_id\", \"datetime_utc\", \"potencia_act\", \"rad_poa\"]], num_strings_inv, on=\"dispositivo_id\").set_index([\"dispositivo_id\", \"datetime_utc\"])\n",
    "    potencia_df[\"potencia_solar\"] = potencia_df[\"rad_poa\"] * potencia_df[\"num_strings\"] * num_mod_string * sup_mod\n",
    "    potencia_df[\"outlier_solar\"] = np.where(potencia_df[\"potencia_act\"] > 0.20 * potencia_df[\"potencia_solar\"], True, False)\n",
    "    disp_df = disp_df.merge(potencia_df[[\"outlier_solar\"]], left_index=True, right_index=True, how=\"inner\")\n",
    "    print(f\"Registros descartados por outlier de potencia: {disp_df[disp_df['outlier_solar'] == True].shape[0]}\")\n",
    "    disp_df = disp_df[disp_df[\"outlier_solar\"] == False].drop(columns = [\"outlier_solar\"])\n",
    "    del potencia_df, num_strings_inv\n",
    "    gc.collect()\n",
    "\n",
    "    # Separación de datos de entrenamiento y validación\n",
    "    train_df, validation_df = train_test_split(disp_df, test_size = 0.2, random_state = 42)\n",
    "\n",
    "    print(f\"\\tRegistros de entrenamiento: {train_df.shape[0]}\")\n",
    "    print(f\"\\tRegistros de validación: {validation_df.shape[0]}\")\n",
    "\n",
    "    # Separación de input y target\n",
    "    target_columns = train_df.filter(like=\"amp_dc\").columns.tolist()\n",
    "    y = train_df[target_columns].copy()\n",
    "    y_val = validation_df[target_columns].copy()\n",
    "    X = train_df.drop(columns = target_columns+[\"id\",\n",
    "                                    \"potencia_act\",\n",
    "                                    \"num_strings\",\n",
    "                                    \"dia_año\",\n",
    "                                    \"hora_seg\"\n",
    "                                    ])\n",
    "    X_val = validation_df.drop(columns = target_columns+[\"id\",\n",
    "                                    \"potencia_act\",\n",
    "                                    \"num_strings\",\n",
    "                                    \"dia_año\",\n",
    "                                    \"hora_seg\"\n",
    "                                    ])\n",
    "    \n",
    "    # Estandarización/normalización de variables numéricas y codificación de variables categóricas\n",
    "    perc_attr = ['cloud_impact', 'consigna_pot_act_planta']\n",
    "    std_attr = ['rad_poa', 'rad_hor', 'temp_amb', 'rad_diff', 'temp_panel']\n",
    "    cat_attr = ['motivo']\n",
    "\n",
    "    transformador_categorico = Pipeline([('onehot', OneHotEncoder(handle_unknown = 'ignore'))])                         # Introducir manualmente catergorías?\n",
    "    transformador_numerico_std = Pipeline([('std_scaler', StandardScaler())]) \n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=[('cat', transformador_categorico, cat_attr),\n",
    "                                                ('std', transformador_numerico_std, std_attr)],\n",
    "                                remainder='passthrough')\n",
    "    \n",
    "    X_prep = preprocessor.fit_transform(X)\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_prep, label=y)\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'tree_method': \"hist\",\n",
    "        'multi_strategy': \"multi_output_tree\",\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    multioutput_model = xgb.train(params, dtrain) \n",
    "\n",
    "    pipeline_model = Pipeline([('preprocessor', preprocessor),\n",
    "                            ('regressor', multioutput_model)])\n",
    "\n",
    "    consulta_sql = f\"\"\"SELECT num_strings\n",
    "                    FROM {schema_name}.distrib_inversores\n",
    "                    WHERE dispositivo_id = {inv_id};\"\"\"\n",
    "    num_strings = pd.read_sql_query(consulta_sql, engine).values.reshape(1, -1)\n",
    "\n",
    "    if optimizacion:\n",
    "        X_val_prep = pipeline_model.named_steps['preprocessor'].transform(X_val)\n",
    "    else:\n",
    "        X_val_prep = xgb.DMatrix(pipeline_model.named_steps['preprocessor'].transform(X_val))\n",
    "    y_pred_val = pd.DataFrame(pipeline_model.named_steps['regressor'].predict(X_val_prep)).rename(columns={i: \"y_pred_\"+str(i+1) for i in pd.DataFrame(pipeline_model.named_steps['regressor'].predict(X_val_prep)).columns})\n",
    "    if normalizacion:\n",
    "        y_pred_val = y_pred_val * num_strings\n",
    "        y_val_reesc = y_val * num_strings\n",
    "    else:\n",
    "        y_pred_val = y_pred_val\n",
    "        y_val_reesc = y_val\n",
    "    \n",
    "    target_pred_df = pd.concat([X_val.reset_index(), y_pred_val], axis=1)[[\"datetime_utc\", \"dispositivo_id\"]+y_pred_val.columns.to_list()] \\\n",
    "                            .melt(id_vars=[\"datetime_utc\", \"dispositivo_id\"], var_name=\"entrada_id\", value_name=\"y_pred\")\n",
    "    target_pred_df[\"entrada_id\"] = target_pred_df[\"entrada_id\"].str.split(\"_\").str[2].astype(int)\n",
    "\n",
    "    y_val_reesc = y_val_reesc.reset_index().melt(id_vars=[\"datetime_utc\", \"dispositivo_id\"], var_name=\"entrada_id\", value_name=\"amp_dc\")\n",
    "    y_val_reesc[\"entrada_id\"] = y_val_reesc[\"entrada_id\"].str.split(\"_\").str[2].astype(int)\n",
    "\n",
    "    prediction_df = target_pred_df.merge(y_val_reesc, on=[\"datetime_utc\", \"dispositivo_id\", \"entrada_id\"])\n",
    "\n",
    "    # Cálculo de las métricas de error\n",
    "    rmse_score = round(mse(prediction_df[\"amp_dc\"], prediction_df[\"y_pred\"], squared=False), 2)\n",
    "    mae_score = round(mae(prediction_df[\"amp_dc\"], prediction_df[\"y_pred\"]), 2)\n",
    "    r2_score = round(r2(prediction_df[\"amp_dc\"], prediction_df[\"y_pred\"]), 3)\n",
    "    metricas = {\"RMSE\": rmse_score, \"MAE\": mae_score, \"R2\": r2_score}\n",
    "    print(f\"\\tMétricas de error:\\n\\t\\tRMSE: {rmse_score}\\n\\t\\tMAE: {mae_score}\\n\\t\\tR2: {r2_score}\")\n",
    "\n",
    "    # path = os.path.join(root_path, f\"Modelos/entrada_amperaje_multi/Inversor_{inv_id - 20}/Repositorio/{model_name}-{pd.Timestamp.now()}/\")\n",
    "    # os.makedirs(path)\n",
    "    # with open(path+'model.model', \"wb\") as archivo_salida:\n",
    "    #     pickle.dump(pipeline_model, archivo_salida)\n",
    "    # with open(path+'informe_modelo.json', 'w') as archivo_json:\n",
    "    #     informe = {\"promediado\": promediado,\n",
    "    #                 \"intervalo_min\": intervalo,\n",
    "    #                 \"normalizacion\": normalizacion,\n",
    "    #                 \"optimizacion\": optimizacion,\n",
    "    #                 \"metricas\": metricas,\n",
    "    #                 \"por_fases\": stage,\n",
    "    #                 \"device\": device,\n",
    "    #                 \"hiperparametros\": pipeline_model.named_steps['regressor'].save_config(),\n",
    "    #                 \"training_input_description\": train_df[perc_attr + std_attr].describe().loc[[\"mean\", \"std\", \"min\", \"max\"]].to_dict(),\n",
    "    #                 \"training_target_description\": train_df[\"potencia_act\"].describe().to_dict()\n",
    "    #                 }\n",
    "    #     json.dump(informe, archivo_json)\n",
    "\n",
    "    # Generación de gráficos: comparativa de valores reales y predichos, histograma de diferencias\n",
    "    y_test_sampled, _,y_pred_sampled, _ = train_test_split(prediction_df[\"amp_dc\"], prediction_df[\"y_pred\"], train_size = 0.25)\n",
    "    plt.figure()\n",
    "    plt.tight_layout()\n",
    "    plt.scatter(y_test_sampled, y_pred_sampled, marker = \".\")\n",
    "    plt.plot([min(y_test_sampled), max(y_test_sampled)], [min(y_test_sampled), max(y_test_sampled)], color='black', linestyle='-', linewidth=1)\n",
    "    plt.xlabel(\"Valores reales\")\n",
    "    plt.ylabel(\"Valores predichos\")\n",
    "    plt.title(\"Comparación de valores reales y predichos\")\n",
    "    # plt.savefig(path + \"scatter_validacion.png\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.tight_layout()\n",
    "    ax = sns.histplot(prediction_df[\"amp_dc\"] - prediction_df[\"y_pred\"], kde=True, stat='percent')\n",
    "    ax.axvline(x=0, color='black', linestyle='--', linewidth = 0.35, label='x=0')\n",
    "    plt.title('Histograma de las diferencias entre valores reales y predichos')\n",
    "    plt.xlabel('Diferencia')\n",
    "    plt.ylabel('Porcentaje')\n",
    "    # plt.savefig(path + \"histogram_validacion.png\")\n",
    "\n",
    "    # Generación de gráficos: comparativa de RMSE y RMSE relativo por entrada\n",
    "    rmse_list = []\n",
    "    rmse_r_list = []\n",
    "    entrada_list = []\n",
    "    for group in prediction_df.groupby([\"dispositivo_id\", \"entrada_id\"]):\n",
    "        entrada_list.append(group[0][1])\n",
    "        rmse_score = round(mse(group[1][\"amp_dc\"], group[1][\"y_pred\"], squared=False), 2)\n",
    "        rmse_r_score = round((mse(group[1][\"amp_dc\"], group[1][\"y_pred\"], squared=False)*100/group[1]['amp_dc'].mean()), 2)\n",
    "        mae_score = round(mae(group[1][\"amp_dc\"], group[1][\"y_pred\"]), 2)\n",
    "        r2_score = round(r2(group[1][\"amp_dc\"], group[1][\"y_pred\"]), 3)\n",
    "        rmse_list.append(rmse_score)\n",
    "        rmse_r_list.append(rmse_r_score)\n",
    "\n",
    "        metricas_entrada = {\"RMSE\": rmse_score, \"RMSE %\": rmse_r_score, \"MAE\": mae_score, \"R2\": r2_score}\n",
    "        \n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Entrada')\n",
    "    ax1.set_ylabel('RMSE', color=color)\n",
    "    ax1.plot(entrada_list, rmse_list, color=color, linewidth=1)\n",
    "    sns.scatterplot(x=entrada_list, y=rmse_list, color=color, ax=ax1)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('RMSE Relativo', color=color)\n",
    "    if rmse_r_score.max() - rmse_r_score.min() > 0.25:\n",
    "        ax2.set_yscale('log')\n",
    "    ax2.plot(entrada_list, rmse_r_list, color=color, linewidth=1)\n",
    "    sns.scatterplot(x=entrada_list, y=rmse_r_list, color=color, ax=ax2)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    plt.title(f'RMSE por entrada para el inversor {inv_id - 20}')\n",
    "    plt.xticks(entrada_list)\n",
    "    plt.tight_layout()\n",
    "    ax1.grid(True, which='major', color='gray', linewidth=0.5)\n",
    "    #ax2.grid(True, which='minor', color='gray', linewidth=0.5)\n",
    "    # plt.savefig(path + \"rmse_entrada.png\")\n",
    "\n",
    "    # Generación de gráficos: comparativa de RMSE y RMSE relativo por hora\n",
    "    rmse_list = []\n",
    "    rmse_r_list = []\n",
    "    hora_list = []\n",
    "    for group in prediction_df.groupby([\"dispositivo_id\", prediction_df[\"datetime_utc\"].dt.hour]):\n",
    "        hora_list.append(group[0][1])\n",
    "        rmse_score = round(mse(group[1][\"amp_dc\"], group[1][\"y_pred\"], squared=False), 2)\n",
    "        rmse_r_score = round((mse(group[1][\"amp_dc\"], group[1][\"y_pred\"], squared=False)*100/group[1]['amp_dc'].mean()), 2)\n",
    "        mae_score = round(mae(group[1][\"amp_dc\"], group[1][\"y_pred\"]), 2)\n",
    "        r2_score = round(r2(group[1][\"amp_dc\"], group[1][\"y_pred\"]), 3)\n",
    "        rmse_list.append(rmse_score)\n",
    "        rmse_r_list.append(rmse_r_score)\n",
    "\n",
    "        metricas_entrada = {\"RMSE\": rmse_score, \"RMSE %\": rmse_r_score, \"MAE\": mae_score, \"R2\": r2_score}\n",
    "        \n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Hora')\n",
    "    ax1.set_ylabel('RMSE', color=color)\n",
    "    ax1.plot(hora_list, rmse_list, color=color, linewidth=1)\n",
    "    sns.scatterplot(x=hora_list, y=rmse_list, color=color, ax=ax1)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('RMSE Relativo', color=color)\n",
    "    if rmse_r_score.max() - rmse_r_score.min() > 0.25:\n",
    "        ax2.set_yscale('log')\n",
    "    ax2.plot(hora_list, rmse_r_list, color=color, linewidth=1)\n",
    "    sns.scatterplot(x=hora_list, y=rmse_r_list, color=color, ax=ax2)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    plt.title(f'Comparativa de RMSE y RMSE Relativo por hora para el inversor {inv_id - 20}')\n",
    "    plt.xticks(hora_list)\n",
    "    plt.tight_layout()\n",
    "    ax1.grid(True, which='major', color='gray', linewidth=0.5)\n",
    "    ax2.grid(True, which='minor', color='gray', linewidth=0.5)\n",
    "    # plt.savefig(path + \"rmse_hora.png\")\n",
    "    plt.close(\"all\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df[(prediction_df[\"dispositivo_id\"] == 21) & (np.abs(prediction_df[\"amp_dc\"] - prediction_df[\"y_pred\"]) > 3*11.17) & (prediction_df[\"amp_dc\"] < 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df[(disp_df.index.get_level_values(\"dispositivo_id\") == 21) & (disp_df.index.get_level_values(\"datetime_utc\") == \"2022-07-18 11:55:00+00:00\")].filter(like=\"amp_dc\").values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df[(disp_df.index.get_level_values(\"dispositivo_id\") == 21) & (disp_df.index.get_level_values(\"datetime_utc\") == \"2022-07-18 11:55:00+00:00\")].filter(like=\"amp_dc\").values.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df[(disp_df.index.get_level_values(\"dispositivo_id\") == 21) & (disp_df.index.get_level_values(\"datetime_utc\") == \"2022-07-18 11:55:00+00:00\")].filter(like=\"amp_dc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df[(disp_df.index.get_level_values(\"dispositivo_id\") == 21) & (disp_df.index.get_level_values(\"datetime_utc\") == \"2022-07-18 11:55:00+00:00\")].iloc[:, 15:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Test_FVPredictive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
